# 第一章 搜索引擎及其基础架构

## 1.1 搜索引擎为何重要

### 1.1.1 互联网的发展

### 1.1.2 商业搜索引擎公司的发展

### 1.1.3 搜索引擎的重要地位

搜索是目前解决信息相对过载的有效方式，在没有更有效的替代方案出来之前，搜索引擎作为互联网网站和应用的入口及处于行业制高点的重要位置会逐步加强。
不论国际还是国内都出现了一种现象，即成功的互联网公司屏蔽搜索引擎公司爬虫，比如脸书对谷歌的屏蔽，淘宝对百度的屏蔽。这种现象是商业公司之间的竞争策略，也是垂直搜索和通用搜索的竞争。

## 1.2 搜索引擎技术发展史

### 1.2.1 史前时代：分类目录的一代

也可以叫做“导航时代”，以Yahoo和hao123为代表。可扩展性不强。

### 1.2.2 第一代：文本检索的一代
经典的信息检索模型，比如布尔模型、向量空间模型或者概率模型。考虑的是query与doc的相关程度，没有利用起网页之间的链接关系。
以AltaVista和Excite为代表。

### 1.2.3 第二代：链接分析的一代
网页链接代表了一种推荐关系，所以通过链接分析可以在海量内容中找出重要的网页，这种重要性本质是对网页流行程度的一种衡量。
缺点：没有考虑用户的个性化需求；存在作弊方案，使得搜索结果变差。

### 1.2.4 第三代：用户中心的一代
目前大部分搜索引擎都输入这一代，需要解决的问题是：如何理解用户很短的query背后包含的真正需求。


## 1.3 搜索引擎的3个目标

- 更全：索引网页的覆盖范围。
- 更快：
- 更准：最核心的竞争力，包括排序、链接分析、反作弊、用户研究等技术。

## 1.4 搜索引擎的3个核心问题

### 1.4.1 三个核心问题

#### (1) 用户的真正需求是什么

从用户需求角度出发：首先query很短，然后不同用户的需求也不同，即使同一个用户在不同场景下的需求也不同。

#### (2) 哪些信息和用户的需求真正相关

在明确用户意图之后，如何找到满足需求的信息？判断query关键词和内容的相关性是这个部分的核心，目前几乎全部该部分的技术全是基于关键词匹配。


#### (3) 哪些信息是用户可以信赖的

内容发布者是否可信？搜索结果可能自相矛盾？比如搜索到的好评可能是商家刷出来的。
链接分析通过网页的重要性作为其可信赖程度的一个标准，在一定程度上可以改善这部分结果。

### 1.4.2 与技术发展的关系

## 1.5 搜索引擎的技术架构

见原书图1-6
后端包括网络爬虫、网页去重，进入云存储与云计算平台的倒排索引模块和链接关系模块。
前段包括查询分析、Cache系统、网页排序、基于倒排索引的内容相似度与基于链接关系的链接分析。
另外，对于所搜引擎的反作弊模块，也成为日益重要的功能。


# 第二章 网络爬虫

目标是下载网页，在本地形成互联网网页的镜像备份。

## 2.1 通用爬虫框架

见原书图2-1。
包括种子URL、待抓取URL队列、DNS解析模块、网页下载器、已抓取URL队列、页面解析器（提取新的URL）。还有可选的网页去重模块和反作弊模块。

对于爬虫的角度，可以将互联网页面分为：已下载网页、已过期网页、待下载网页、可知网页、不可知网页。

商业爬虫基本都属于增量式爬虫，这需要需要解决网页的更新问题，包括新增、内容更改、网页删除。另外垂直爬虫需要在线识别内容相关性，检测是否属于某个垂直领域。

## 2.2 优秀爬虫的特性

实用的爬虫系统应当具备的特性：

- 高性能：单位之间内下载的网页数量。涉及到访问磁盘的方法、待抓取和已抓取的队列的数据结构。
- 可扩展性：可以通过抓取服务器和爬虫数量形成分布式爬虫。
- 健壮性：被抓对象具有多样性，另外如果爬虫自己宕机，重启后也应可以继续运行。
- 友好性：保护网站的的部分私密性；减少被抓取网站的负载。爬虫禁抓协议指robot.txt，通常针对目录级别。网页禁抓标记可以到网页级别的粒度。

## 2.3 爬虫质量的评判标准

- 抓取网页覆盖率：影响搜索引擎的召回率。
- 抓取网页时新性：
- 抓取网页重要性：影响搜索引擎的精度。

我们基本不可能获取全网的网页，那么我们的原则是：有限的资源下，尽可能的选择比较重要的页面进行索引，已抓取的部分尽快更新；在此基础上，扩大抓取范围。

大型商业搜索引擎会针对不同的标准部署多套搜索引擎。

## 2.4 抓取策略

待抓取URL队列的顺序是如何确定的。基本策略是优先选择重要的网页进行爬取，重要性通常使用流行性进行定义。

- 宽度优先：banchmark。虽然提出时间很早，也看似很机械，但是实际上效果不错而且基本上是按照网页的重要性作为顺序。一种解释方法是，如果某个网站有很多入链，那么就可以认为它很重要，并且更有可能更早的进入队列。
- 非完全（Partial）PageRank策略：在已下载网页和待下载网页的页面子集中运行PageRank，对于新来的页面，不一定将它放到页面最后，而是通过其入链先赋予一个临时PageRank评分。效果不一定优于BFS。
- OCIP（Online Page Importance Computation）：每个页面具有相同的初始评分，当页面被下载之后就将自己的评分全部给出去，不需要迭代，可以实时使用。（看书中的疑问，我觉得这里应该用优先队列、堆来实现？但是要每次评分变动后实时更新？那么就是索引堆？）相比Pagerank没有随机跳转，效果略优于BFS。
- 大站优先策略：哪个网站等待下载的页面最多，则优先下载。效果略优于BFS。

## 2.5 网页更新策略

本地页面就是互联网页面的镜像，更新就是要保证一致性。网页更新策略的任务是决定何时重新抓取之前已经下载过的网页。

- 历史参考策略：利用泊松过程，根据历史网页更新情况，对网页的变化进行建模利用模型预测可能何时发生变化。
- 用户体验策略：如果该页面不影响搜索引擎的效果，那么就无关紧要，即采用搜索结果的排名进行衡量。通常对于网页历史不同版本在搜索引擎的排名的平均值作为依据。
- 聚类抽样策略：优点是无需历史信息，历史信息存在存储代价大和冷启动的问题。效果略好于前两种策略，但是对于大量网页进行聚类有计算难度。因此有的策略直接忽略聚类这个步骤，然后一个网站下的所有页面都是一个聚类。

## 2.6 暗网抓取

通常是某些网站的数据库查询页面，比如查询电商的具体商品信息或者航班信息等等。

### 2.6.1 查询组合问题

Google提出了ISIT算法来搜索最富含信息的查询模板，这个算法很类似于Apriori算法。

### 2.6.2 文本框填写问题

提供种子词，然后根据返回结果进行扩展。

## 2.7 分布式爬虫

### 2.7.1 主从式分布式爬虫

需要URL服务器提取URL分发服务，维护待抓取队列，进行worker之间的负载均衡。

### 2.7.2 对等式分布爬虫（Peer 2 Peer）

Mercator爬虫就是此结构。通过对url的哈希判断是否应该由自己来承担该url的抓取，如果不是自己，则发送给对应的服务器。哈希函数通常针对主域名，这样每个服务器一方面可以利用起DNS缓存，另一方面可以针对性的控制对某个网站的访问速度。

如果对分布式比较熟悉就会发现此时哈希函数对于服务器数量不稳定，UbiCrawler的改进方案就是使用一致性哈希协议。

# 第三章 搜索引擎索引

## 3.1 索引基础

### 3.1.1 单词文档矩阵

单词文档矩阵是一种共现关系的概念模型。搜索引擎的索引就是这种共现关系的具体数据结构，倒排索引不是上述概念模型的唯一实现方式，但是是数据表明最好的方式。

### 3.1.2 倒排索引基本概念

倒排索引通过文档id和单词id来构建，倒排索引主要由两个部分构成：单词词典和倒排列表。倒排列表由倒排项构成，包括位置信息。所有单词的倒排列表顺序的存储在磁盘的某个文件中，倒排文件是倒排索引的物理文件。（疑问：单词词典不需要落盘吗？另外如果倒排列表在磁盘中，是不是要考虑访问局部性的问题，利用缓存？）

### 3.1.3 倒排索引简单实例

## 3.2 单词词典

- 3.2.1 哈希表（拉链法）
- 3.2.2 B树或B+树

## 3.3 倒排列表

在实际的搜索引擎系统中，并不存储倒排索引项的实际文档编号，而是代之以文档编号差值（D-Gap），通常可以保证D-Gap大于零。好处是索引压缩。

## 3.4 建立索引

### 3.4.1 2-Pass In-Memory Inversion

第一遍遍历文档获取全局统计信息，包括：文档个数、词典大小、单词的文档计数DF。通过D这些信息可以知道总共所需的内存大小（因为假设一个单词的DF是10，可以知道这个单词的倒排列表中有10个倒排项）。通过这些信息可以申请一个连续的内存空间，然后词典中的每个词对指向自己的到排列表的对应的开始和起始位置。第二遍遍历填充第一步开辟的空间，对于每一个单词，需要填充文档ID，单词计数TF。

从磁盘读文档的操作比较耗时（该方法要两遍），另外该方法吃内存，所以不常用。

### 3.4.2 Sort-Based Inversion

该方法只需要固定大小的内存。一次读取每一篇文档，可以得到这篇文章中所有单词对于这篇文章的倒排项`<wordID, docID, TF>`。把这一堆三元组放入缓存区。缓存区被填满后，对缓存中所有的三元组排序，排好序后写入临时文件。所有的文档扫描一遍后，合并所有这些临时文件得到最终索引。注意，词典是一直维护在内存中的并且不断变大，因此，缓冲区会越来越小。

因为临时文件内部关于wordID排序，因此，可以依次读取每一个单词的所有倒排项，得到所有倒排项后，将该单词的倒排列表写入文件，然后该单词就搞定了。

### 3.4.3 归并法

归并法对Sort-Based Inversion的一个缺点进行了改进，就是随着词典变大，缓冲区越来越小这个问题。从具体实现上，二者存在较大区别：

1. 中间过程中，排序法存的是三元组的列表，归并法在缓冲去存放的就是完整的索引结构，不过是针对部分文档的。
2. 包括词典也要写入磁盘，因此这里词典不能够再为一个词赋予一个id了，因为下次重新建词典时，词可能会被分配到新的id，这里词典只保存次本身的信息比如DF和指向到排列表的指针。
3. 合并的时候不是合并三元组，而是合并一个个的部分到排列表。然后还需要重建词典，包括DF和倒排列表指针。

## 3.5 动态索引

大多数搜索引擎维护的是一个动态的文档库、因此为了维护这种更新变化，动态索引除了倒排索引，还需要两个部分：临时索引和已删除文档列表。将临时索引维护在内存中，已删除文档列表维护在磁盘中。更新文件看作是删除旧文件且新增新文件。

## 3.6 索引更新策略

### 3.6.1 完全重建策略
新增文档达到一定数量时，对所有文档重新建立索引。(看起来代价很高，但是书上说目前互联网大部分采用此方案，没有说为啥。)重建过程中，使用老索引响应查询。

### 3.6.2 再合并策略
思路也比较直接。（问题：因为直接将倒排列表合并，此时不能保证文档顺序，也就是不能保证D-Gap始终为正？）维护过程中，使用老索引维护查询。

### 3.6.3 原地更新策略
思路是解决在合并策略的一个问题：如果有大量没有改变的词，那么更新的时候也要重新把他们写入新索引，没有必要。如果可以的话最好只把有新增的词的倒排列表追加到旧索引的对应词的后方。
方式是每一个词的倒排列表后面预留出空闲位置，直到空闲位置不足时，将该单词的倒排列表迁移到另外的连续空间中。这种方法，思路很好但是效果不咋地。因为实际使用中会经常迁移，磁盘需要对磁盘的可用区域进行管理，导致代价比较大。另外，此时倒排文件不再是单词有序的，合并操作就需要词典辅助来找到每个单词的倒排列表的位置（方法二的合并不需要词典辅助）。

### 3.6.4 混合策略（Hybrid）
对于常见的单词，倒排列表很长，会频繁更新，使用原地更新策略，原因是原地更新策略节省磁盘的读写次数。不常见的单词，使用再合并策略，可以利用起索引文件的顺序读写性能。

## 3.7 查询处理

### 3.7.1 一次一文档
假设需要输出前k个结果，那么使用一个长度为k的优先队列就可以。

### 3.7.2 一次一单词
需要的数据结构是哈希表，因为计算后续单词时，此时的文档得分可能还会提高。

### 3.7.3 跳表（skip-pointers）
思路很不错，就是说如果倒排列表使用D-Gap并且以压缩编码存在磁盘上，那么求一个文档的倒排项就比较麻烦需要整体解压。使用分块的思想可以快速查找并且只对一个块进行解压。（问题：但是合并怎么办？而且这种方法需要保证一定倒排列表关于文档有序才行啊。）

## 3.8 多字段索引

- 多索引方式
- 倒排列表方式
- 扩展列表方式

## 3.9 短语查询

### 3.9.1 位置信息索引
单词的倒排列表长度会急剧增长，一方面消耗存储空间，另一方面影响磁盘的读取效率。而且对于较长的短语，计算代价比较高。

### 3.9.2 双词索引

### 3.9.3 短语索引

挖掘出热门短语，然后之间对这些短语建立倒排索引。

### 3.9.4 混合方法

位置索引适合常规的短语查询，双词索引适合处理计算代价高的短语查询（比如停用词构成的短语），短语索引适合处理热门短语。

## 3.10 分布式索引

通常有两种方案：**按文档划分索引**和**按单词划分索引**，按文档的划分方式比较常用原因在于：

- 可扩展性：很容易支持新增的文档。而按单词划分的话对于新增的文档可能会对所有的服务器节点都有影响。
- 负载均衡：有些单词比较常见，导致可能会被大量调用。
- 容错性：假设某台服务器宕机，如果是按照单词划分，那么在用户查询包含该单词的时候可能导致没有结果，然而如果是按照文档划分，可能结果中会少召回一些文档但是不影响正常使用。

# 第四章 索引压缩

## 4.1 词典压缩
词典通常放到内存中，一个词典项通常要包含单词本身、DF和到排列表指针。

这部分内容让我想到了操作系统中的文件系统。思想主要是单词是变长的，不能按照最长的单词分配长度，索性把单词写到一起，然后只记录定长的指针。分块的思想可以再省出若干个指针，但是要在连续字符串中加入长度或者分割符信息。

## 4.2 倒排列表压缩算法
倒排列表通常包含3类信息：文档编号、词频信息以及单词位置序列信息。**其中文档编号和单词未知序列是依次递增的，所以通常存储其差值来将大整数转换为大量的小整数**。（问题：但是也不能确保没有大整数的间隔啊。。所以怎么确定数据类型或者说字长。）到排列表的特点是：数字分布严重不均衡，小数值占了很大的比例。

### 4.2.1 评价索引压缩算法的指标

- 压缩率
- 压缩速度：不那么重要，因为压缩过程不多，基本是在建立索引的时候完成。
- 解压速度：最重要的指标。

### 4.2.2 一元编码与二进制编码

### 4.2.3 Elias Gamma 压缩算法与 Elias Delta 算法

### 4.2.4 Golomb 算法与 Rice 算法

### 4.2.5 变长字节算法

### 4.2.6 SimpleX系列算法
该系列最常见的算法是Simple9，在此基础上有改进算法Simple16，relate10，carryover12等。

### 4.2.7 PForDelta算法
是目前解压速度最快的一种倒排文件压缩方法，

## 4.3 文档编号重排序
该技术的核心目的是重新编排文档的编号，以使得某个单词倒排列表中相邻的两个文档其编号尽可能相近。

## 4.4 静态索引剪裁
有损压缩方式。
将不重要的索引项去掉，用户也基本感受不到索引不完整。

# 第五章 检索模型与搜索排序

检索模型假设查询词已经明确的表达了用户的意图，因此检索模型不牵扯对用户需求建模。

## 5.1 布尔模型

## 5.2 向量空间模型

### 5.2.1 文档表示
tfidf

### 5.2.2 相似性计算
cosine相似度，有一个缺点是倾向于段文档，因为长文档通常涉及到的词或者主题较多，规范化后，每个特征的权重可能都不会特别大。

### 5.2.3 权重特征计算
tfidf框架

总的来说，向量空间模型非常成熟，且被广泛采用，但是总体而言，向量空间模型，是一个经验型的模型，靠直觉的经验不断摸索完善，缺乏一个明确的理论来指导其改进方向。

## 5.3 检索概率模型
概率模型是目前效果最好的模型之一，okapi BM25这一经典概率公式已经在商业搜索引擎的网页排序中广泛使用。

### 5.3.1 概率排序原理

基本思想是给定一个用户查询，如果搜索系统能够在排序时按照用户需求和文档的相关性排序，我们要做的就是估计这种相关性。注意向量空间模型是用相似性作为相关性的代理。

### 5.3.2 