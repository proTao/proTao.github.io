

## 原理部分
1. 取值很大的数据（比网络权重初值大很多）和没有标准化的数据一样不安全，可能导致大的梯度使网络不能收敛。
2. 如果数据点有冗余，要让他们都出现在训练集中而不是有的在训练集有的在测试集，要确保训练集和测试集没有交集。
3. 一般来说，对于神经网络，将缺失值设置为0是安全的。网络能够从数据中学到0意味着缺失数据，并且会忽略这个值。注意，如果测试数据中可鞥有缺失值，而网络实在没有缺失值的数据上训练的，那么网络不可能学会忽略缺失值。在这种情况下，你应该人为生成一些有缺失项的训练样本：多次复制一些训练样本然后删除测试数据中可能缺失的某些特征。
4. 一个K-fold Validation的变体，多次使用K折交叉验证并且每次打乱数据。
5. 在验证集上进行模型选择本身就是一种超参数的学习。
6. 对于简单的回归任务，使用MAE作为metric，使用MSE作为loss。MAE具有更好的可理解性。
7. 中间层的neuron数不能少于多分类任务的类别数目。
8. 多分类任务可以不用onehot对标签编码，直接使用keras中的sparse_categorical_crossentropy进行交叉熵的计算。
9. 对于序列分类问题，可以用联结主义时序分类损失函数（CTC）。
10. 具有多个输出的神经网络可能具有多个损失函数，每个输出对应一个损失函数。但是梯度下降过程必须基于单个标量进行。因此此时需要将所有的损失函数取平均，变成一个标量值。
11. 在keras中，贯序Sequential模型是最简单的结构，可以使用函数式API定义其他变体网络拓扑，比如双分支、多头、Inception模块。
12. TensorFlow是一种使用符号微分（Symbolic Differentiation）的现代框架来实现的神经网络工具。
13. Momentum解决了SGD的两个问题：收敛速度和局部极小。
14. 最好记住，低维表示中形成的直觉在实践中不一定适用于高维空间，这在历史上一直是深度学习研究的问题来源。
15. 图像张量的形状有两种约定，channels-last（Tensorflow）和channels-first（Theano）
16. 对于数据张量，第 0 轴被称为批量轴（batch-axis）。
17. 深度学习的好处是将多阶段流程替换为一个简单的、端到端的深度学习模型。在实践中两个利器是GBM用于浅层学习，DeepLearning用于感知问题。
18. 特征工程的本质：不是一味地加入更多的特征，而是用更简单的方式表述问题，从而使问题变得更容易。通常需要深入理解问题。深度学习可以自动提取出有用的特征，但是这不意味着特征工程不重要了，因为：（1）良好的特征可以使用更少的资源更优雅的解决问题。（2）良好的特征可以用更少的数据解决问题，因为深度学习需要大量的数据。
19. 机器学习的根本问题是优化和泛化之间的对立。最优的解决方法是获取更多的数据。
20. 在深度学习中，模型的capacity指的是可学习参数的个数。
21. 关于Occam's razor原理，就记住一句话，一个事情的两种解释，简单的那个更可能正确。即假设更少的那一个。具体到机器学习中，什么叫更简单的模型？要么就是参数值的熵更小，此时我们用正则化。要么就是参数更少，那就选用VC更低的模型，还有dropout。（注：可以从模型集成的角度理解dropout）
22. 对于正则化，一定要注意，还增加了模型的bias，就好像“三短一长选最长”。在keras中正则化项只在训练时计算，因此训练损失要比测试损失大。
23. 测试时，dropout层的输出值要按照dropout rate缩小。
24. 实践中，类似于简单统计机器学习使用最简单的线性模型开始做起，深度学习也要先搭建一个很简单的略优于随机模型的模型，这被称为获得**统计功效(statistic power)**，并成为我们模型的baseline。然后类似于使用零值定理寻找零点，我们需要从欠拟合跨越到过拟合并逐渐去逼近二者平衡的那个临界值。但是注意，这个过程是迭代进行的，我们通过验证集对模型进行评估，反复使用验证集后，机会发生信息泄露，最终使得模型对验证集的过拟合。
25. 接上一点，对于一个简单、可理解并且鲁棒的基准模型十分重要，这意味着我们在后续使用更复杂的模型时，可以使我们有信心说，继续增加模型的复杂度合理并且有收益的。比如9比1的不平衡问题，我们的基准分类模型就是0.9的正确率，因为我们可以简单的预测所有的样本都是大类而得到90%的正确率，这个基准在有的时候并不是那么容易达到。在LSTM一章还有一个例子就是预测气温，简单的使用昨天此时的温度来预测可以达到相当好的效果，实验中一个全连接的wangluo网络没有能达到这个基准，尽管模型空间中涵盖了这个简单的预测方法。**因为，如果学习算法没有被硬编码要求去寻找特定类型的简单模型，那么有时候参数学习是无法找到简单问题的简单解决方案的。**
26. Normalization 是一大类方法，用于让机器学习模型看到的不同样本在彼此之间更加相似，这有助于模型的学习与对新数据的泛化。BN层的原理是，训练过程中在内部保存已读取每批数据均值和方差的指数移动平均值。其主要成果是有助于梯度传播。通常在卷积层或者`Dense`层之后使用。
27. **深度可分离卷积**（depthwise separable convolution），让模型更加轻量（参数少），速度更快（浮点数计算少），任务性能还可以提升。方法就是先对输入的每个通道单独进行空间卷积，然后得到的层叠加起来，在进行1×1的通道卷积。这相当于将空间特征学习与通道特征学习分离开。可以认为是卷积层的无痛替代产品。


## 图像与卷积网络
1. Dense层和Conv层最主要的区别在于Dense从输入学习全局模式，Conv层学到的是局部模式。这使得有两个衍生特性，一个是卷及神经网络的模式具有平移不变性（先不说这个模式的缺点，比如Hitton提出的capsule），实际上就是权重共享，各个位置的局部窗口共享卷积核，这使得模型对数据的利用更高效，泛化能力更强。其次就是特征具有层次结构。注意这种平移不变性实际上是导致最后得到的 1×1×n 的feature map中就是一个感知特定结构是否出现以及出现强度的特征，后面加dense意味着这个特征向量中元素的位置不重要。
2. 卷积层的输出是特征图，也叫响应图。
3. padding的参数是"valid"表示不填充，"same"表示使输出和输入的大小一致，默认值是"valid"。
4. 对于卷积设置stride=2，等于是做了二倍的下采样，实践中大于一的步长用的不多，想要下采样的话使用pooling代替。pooling同样不影响所谓的平移不变性。考虑一个人脸图片，眼睛的位置上是嘴，嘴的位置上是眼睛，人类看这个图片会知道这不是人脸，但是卷积神经网络会认为这个有很大概率是人脸。
5. pooling的主要作用是，（1）扩大高层neuron的感知野。（2）减少最后的feature map的元素个数。一种可能的代替方案是，可以使用带孔卷积核（dilated kernel），同样可以在不增加参数量的情况下扩大感受野。
6. 对于图片推荐使用 keras 中的`keras.preprocessing.ImageDataGenerator`，可以批量的指定预处理操作。其中的`flow`方法对一个图片样本构成的batch进行指定预处理，`flow_from_directory`以文件夹路径作为输入参数，返回一个无限生成器。然后使用`fit_generator`直接接受一个数据生成器并开始训练。这个`ImageDataGenerator`还可以方便的完成Data Augment。
7. 使用预训练的网络进行迁移学习是深度学习的一个重要优势。使用预训练网络有两种方法：**特征提取**(feature extraction)和**微调模型**(fine-tuning)。特征提取就是复用已有模型的卷积核，越低层的卷积核得到的特征越具有通用性，而Dense层通常不能复用。特征提取就是完全不改变预训练模型的卷积基，只利用卷积基提取特征或者在卷积基后面接一个自己的dense+dropout分类器，后者的好处是可以使用data-augment。fine-tuning是在后面接上自己的分类器并在训练好之后，解冻模型的最后几层卷积层，再训练几轮。
8. 可视化卷积网络有三个可能的方向：（1）可视化response map。（2）可视化filter。（3）可视化类激活的热力图。

## 文本序列与LSTM
1. Keras内置的`keras.preprocessing.text`可以对文本进行预处理。其中Tokenizer类，
2. SimpleRNN 非常好理解。
3. 对于Embedding层，可以使用预训练好的 embedding 向量，也可以自己训练。类似于 CNN 的预训练，如果是使用已经训练好的 embedding 层，需要在训练时将其冻结，可以在最后解冻并进行fine-tuning。
4. GRU是2014年提出的对LSTM的一种简化。
5. 在dropout可以加到循环神经网络内部。但是每个时间步的dropout掩码应该一致。
6. 双向 RNN 就是同时有两个RNN，不过一个的输入序列是正常的序列，另一个是反序。比如序列是12345，其中第二个RNN用54321来训练，然后输出对两个RNN进行合并。对于双向RNN不要盲目，比如对于温度预测的任务，可以先用反向RNN进行试验，发现效果大打折扣，这很容易理解，因为越近的温度越有预测意义，那么双向RNN可能对问题没有帮助。而对于文本，由于结构性，词语的重要性并不是与位置正相关，使用反向RNN进行测试，发现得到了类似的分数，但是由于二者的表示很不相同，类似于集成学习的思想，使用双向RNN对于我们的任务就会有帮助。
7. recurrent attention 和 sequence masking 是 NLP 的deeplearning 中非常有用的技术。
8. 对于卷积神经网络，也可以应用到序列建模中，不过卷积窗变成一维的向量。Conv1D具有同样具有全局的平移不变性，这使得对于温度预测来说Conv1D仍然不是一个好方法，因为靠后的数据和靠前的数据重要性不一样，但是卷及神经网络却同等对待，但是对于情感分析来说，Conv1D完全可以胜任，因为关于情感的词出现在句子中哪个位置，通常都是一样的。
9. CNN的好处是比循环神经网络更快，因此可以考虑叠加CNN与RNN，在RNN前面使用CNN，缩短长序列，将CNN作为特征提取的手段，CNN可以将长的输入序列转换为高级特征组成的更短序列。然后将CNN的输出作为RNN的输入，就能有效的利用起CNN的**速度**和RNN的**顺序敏感性**。


## 未归类
1. 使用LSTM或者GRU
2. 循环状态中涉及到矩阵的反复乘法，因此涉及到这种操作的矩阵初始化为正交矩阵很有帮助。
3. 其他的参数初始化为很小的值，一旦值过大，就会导致激活后的导数很小，这就容易使得神经元“死亡”。
4. LSTM中，对于forget gate的bias设置为1，或者其他大小合适的正数。这意味着我们期望默认的初始状态，模型能更倾向于记住东西（这么一想，似乎叫做don‘t forget gate更合适一点）。因为学者发现LSTM的关键因素是遗忘门,而 Jozefowicz et al. (2015) 发现向 LSTM 遗忘门加入 1 的偏置(由 Gerset al. (2000) 提倡) 能让 LSTM 变得与已探索的最佳变种一样健壮。
5. 更适合自适应学习率的算法，比如Adam，Adadelta等。
6. 防止梯度爆炸，对梯度进行剪裁（1，或者5都是合理的值）。
7. 在垂直方向上进行dropout。所谓垂直，指的是，如果是多层的RNN，那么在不同层之间加上dropout，而不是在同一层的不同时间步上进行dropout。在水平方向上进行dropout也不是不合理，但是方式要更精巧一些，比如以固定的掩码的方式等等。
8. curriculum learning，是机器学习中的一个训练策略，简单说就是先学习简单训练数据，再学习复杂训练数据。
9. 可以使用预训练好的词向量用于具体NLP任务中，如果数据足够，可以继续训练词向量，这样可以使得词向量的表示更贴合当前任务，比如good和bad在单纯的word2vec中位置可能很近，但是用于情感分析的任务中并继续训练词向量可以将good和bad分得更开。
10. hierachical softmax的问题在于树结构的计算形式无法利用GPU的计算能力。
11. 为什么正则项不考虑bias？bias项的目的是帮助预测向类分布的方向倾斜，从而起到非均匀先验的作用。添加正则化会阻止bias的这种功能。
12. 二分类任务时，在model最后一层使用max-margin损失实际上是结合了SVM。
13. 使用孪生网络（Siamese Network）刻画相似性，更好的方式是Triplet Network，模型的能力更强。与Siamese Network一样，Triplet Network适用于解决样本类别很多（或不确定），然而训练数据集的样本数又很少的情况（如人脸识别、人脸验证）。
14. 在使用基于窗口的模型时，窗口内的多个词向量可以连接，可以求和，还可以使用autoencoder的方式，这个方式是在cs2242017的期中复习的第三篇文档上看到的。需要在损失函数中加上重构损失。好处在于如果窗口内的词是“not bad”，词袋模型可能会认为这是一个负向的词。而重构损失防止网络忽略其中一个x1或x2的贡献。换句话说，它迫使网络捕捉x1和x2的“联合意义”。
15. sigmoid函数具有“压缩性”，忘了这个词对不对了。。。就是说不停迭代x=f(x)，最后x会趋于定值，正弦、余弦函数都具有类似特性。
16. 一种对于softmax的正则化方法：对于每一个类的参数向量（就是softmax参数矩阵的一行），限制其最大norm是s，如果范数大于s了，就缩放其norm。不是很常用的方法。
17. 在RNN中，初始化很重要，甚至初始化的顺序很重要。在cs224的作业中，颠倒W和U的初始化顺序会影响是否出现梯度爆炸现象。
18. 对于序列建模使用循环神经网络，如RNN。递归神经网络代表循环网络的另一个扩展,它被构造为深的树状结构而不是 RNN 的链状结构,因此是不同类型的计算图。递归网络的一个明显优势是,对于具有相同长度 τ 的序列,深度(通过非线性操作的组合数量来衡量)可以急剧地从 τ 减小为 O(logτ)，这有助于解决长期依赖的问题。
19. 由每个节点执行的计算无须是传统的人工神经计算(所有输入的仿射变换后跟一个单调非线性)。节点不一定是传统的单层线性加元素级非线性的运算构成，张量运算和双线性形式（cs224中提到双线性是uWv这种形式是双线性，是在介绍attention的计算时提到的）也是可选的节点的运算方式，在这之前人们已经发现当概念是由连续向量(嵌入)表示时,这种方式有利于建模概念之间的联系。
