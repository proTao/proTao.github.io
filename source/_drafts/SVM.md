## 零碎知识点
1. 支持向量机不提供概率输出,而是对新的输入进行分类决策。
2. 需要特征规格化。
3. 线性可分情况下，最优解可证唯一。《统计学习方法》P100。
4. 间隔长度是w模长的倒数的2倍。
5. 最终的解只依赖支撑向量。
6. 惩罚系数C越大，SVM越倾向于将每个点都分类正确；C越小，SVM泛化能力越强，能容忍更多的错误点。
7. 结构风险最小化来提高泛化能力。

## Perceptron
1. 判别模型，目标是找到一个超平面将特征空间中的点分开，决策函数与SVM相同。是神经网络和SVM的基础
2. 学习策略：假设训练集是线性可分的，学习目标是求得一个能够将训练集正负类完全区分的的分离超平面，为了找的这样的超平面。定义**经验损失函数**并极小化损失函数。
3. 损失函数：直观的选择是误分类点的个数，但是该函数难以优化，所以选择修改该损失为误分类点到超平面的距离和，此时损失函数是超平面参数w和b的连续可导函数。
4. 训练算法：随机梯度下降。（不是使得所有的误分类点梯度下降，而是随机选择一个误分类点使其梯度下降。）
5. 收敛性证明：Novikoff定理。（具体证明略过）当训练数据集线性可分的时候，感知机学习算法是收敛的，感知机算法在训练数据集上的误分类次数k存在上界$$$(\frac{max||x||}{\gamma})^2$$$。
6. 线性可分的数据集上，perceptron存在无数多个解，由于不同的初值或者迭代顺序导致不同。
7. 学习算法的对偶形式
8. 优点：计算和概念都十分简单；因为是随机梯度下降，每次选择一个点，所以是常数的内存消耗，并且可以在线学习（在线学习使得可以扩展至大规模数据）。
9. 缺点：非线性可分数据不能收敛；准确度不够；模型过时。
