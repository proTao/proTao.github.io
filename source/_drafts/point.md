## SVM
1. 支持向量机不提供概率输出,而是对新的输入进行分类决策。
2. 需要特征规格化。
3. 线性可分情况下，最优解可证唯一。《统计学习方法》P100。
4. 间隔长度是w模长的倒数的2倍。
5. 最终的解只依赖支撑向量。
6. 惩罚系数C越大，SVM越倾向于将每个点都分类正确；C越小，SVM泛化能力越强，能容忍更多的错误点。
7. 结构风险最小化来提高泛化能力。
8. 非概率模型

## Perceptron
1. 判别模型，目标是找到一个超平面将特征空间中的点分开，决策函数与SVM相同。是神经网络和SVM的基础
2. 学习策略：假设训练集是线性可分的，学习目标是求得一个能够将训练集正负类完全区分的的分离超平面，为了找的这样的超平面。定义**经验损失函数**并极小化损失函数。
3. 损失函数：直观的选择是误分类点的个数，但是该函数难以优化，所以选择修改该损失为误分类点到超平面的距离和，此时损失函数是超平面参数w和b的连续可导函数。
4. 训练算法：随机梯度下降。（不是使得所有的误分类点梯度下降，而是随机选择一个误分类点使其梯度下降。）
5. 收敛性证明：Novikoff定理。（具体证明略过）当训练数据集线性可分的时候，感知机学习算法是收敛的，感知机算法在训练数据集上的误分类次数k存在上界$$$(\frac{max||x||}{\gamma})^2$$$。
6. 线性可分的数据集上，perceptron存在无数多个解，由于不同的初值或者迭代顺序导致不同。
7. 学习算法的对偶形式
8. 优点：计算和概念都十分简单；因为是随机梯度下降，每次选择一个点，所以是常数的内存消耗，并且可以在线学习（在线学习使得可以扩展至大规模数据）。
9. 缺点：非线性可分数据不能收敛；准确度不够；模型过时。
10. 非概率模型

## KNN
1. 是一种基本的分类与回归方法。k值的选择，距离度量以及分类决策规则是k近邻法的三个要素。
2. KD-Tree

## NB
1. 生成模型

## Decision Tree
1. [CART](cart.jpg)
2. 先剪枝：
- 树到达一定高度
- 节点下包含的样本点小于一定数目
- 信息增益小于一定的阈值等等
- 节点下所有样本都属于同一个类别
3. 后剪枝方法：
- 降低错误剪枝 REP(Reduced Error Pruning)
- 悲观错误剪枝 PEP(Pessimistic Error Pruning)
- 基于错误剪枝 EBP(Error Based Pruning)
- 代价-复杂度剪枝 CCP(Cost Complexity Pruning)
- 最小错误剪枝 MEP(Minimum Error Pruning)
CART用的就是CCP剪枝,其余的剪枝方法可以网上google一下. CCP剪枝类实际上就是我们之前讲到的最小化结构风险。结构风险中的正则项在决策树中是叶节点数目。
4. 决策树缺点：不支持在线学习,有新样本来的时候,需要重建决策树.
5. C4.5支持缺失值处理：
- 赋上该属性最常见的值
- 根据节点的样例上该属性值出现的情况赋一个概率,比如该节点上有10个样本,其中属性A的取值有6个为是,4个为否.那么对改节点上缺失的属性A,以0.6的概率设为是,0.4的概率设为否
- 丢弃有缺失值的样本
6. C4.5支持连续值属性：把需要处理的样本(对应根节点)或样本子集(对应子树)按照连续变量的大小从小到大进行排序.假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点,根据这个分割点把原来连续的属性一分为二.但实际上可以不用检查所有N−1个分割点,如果相邻两个值对应的类别一样，那就不用尝试该分割点了，当然回归问题就要挨个尝试了。


## LR和最大熵

1. LR与朴素贝叶斯的联系
- 相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)P(Y=1|X)形式跟逻辑回归是一样的。
- 不同的地方在于，逻辑回归为判别模型求的是p(y|x)p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。

2. LR与最大熵模型
逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。
- 指数簇分布的最大熵等价于其指数形式的最大似然。
- 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；
- 多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。