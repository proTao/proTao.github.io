---
layout: post
title: Spark笔记（二）
category: 大数据
tags:
- bigdata
- spark
- dataprocessing
keywords: scala spark bigdata
description:
---


- 日期：20180402
- 工作目标：执行C1实时处理的spark任务
- 上下文：批处理没有数据，实时处理部分kafka上有数据，我就好像一个运维加测试人员，还是很差的那种，马卫师兄作为一个研发人员，给我代码，我跑一跑，测试一下、

spark-submit命令后有错.

使用yarn application --list查看当前任务已经shutdown
在启动log中查看分配的applicationID： application_1521425076696_0157
使用yarn logs -applicationId application_1521425076696_0157查看yarn日志
发现是配置文件中kafka topic配置的问题，这部分是由马卫师兄直接生成的所有省份的日志



* * *



- 日期：20180403
- 工作目标：了解当前集群资源情况，多跑几个job
- 上下文：上午和马卫师兄沟通一下代码，下午准备多跑几个任务，先跑起来再说，但是要考虑到集群上的资源

- 进展：没有头绪，暂且搁置，复习spark


关于web界面，配置host

* * *



- 日期：20180403
- 工作目标：熟悉一下spark，跑几个wordcount，了解大致程序结构、打包方式。

1. 在spark-shell中发现默认的hdfs路径是hdfs://hadoop-master01-crs2:8020/user/cdn/SecurityConfig\_shanghai.xml
2. spark集群是on yarn，master地址是yarn-sluster
3. 第一个python脚本。提交：`spark-submit --master yarn --deploy-mode client script.py`


* * *

- 日期：20180408
- 工作目标：完成测试框架脚本
- 结果：主要工具是expect。已完成
- 主要问题：需要经常查阅shell脚本命令。还有就是expect语法不熟练，tcl语言基本就是不会，但是这次使用基本巩固了expect的基本用法，学会了使用expect的正则方式并捕获命令输出传递给环境变量，还有一些零碎的小trick，比如xargs和cut命令的结合使用；在expect中log_file将输出分流到文件；puts命令输出当前expect环境变量，stderr方便的使用标准错误输出。

- 工作目标：封装expect脚本，标准化数据输出.
- 结果：已完成.

- 工作目标：封装exe脚本，批量化数据输出.
- 结果：已完成.


* * *

- 日期：20180409
- 工作目标：理解参数，在大规模数据上运行

问题1：测试数据是纯文本，给的数据是压缩的，用手动处理吗
回答：spark的textFile可以直接读取gz文件，这个文件可以任意大小，我的有2个多G，不用担心OOM，因为这里不做处理，所以没关系

问题2：随便跑了一个小文件，看不见输出？
回答：粗心。命令中有两个地方需要设置配置文件，我只改动了一个

问题3：得到hdfs上一个文件夹的空间和多个文件的大小
`hdfs dfs -du -s -h /coll_data/dns/beijing100/20180407/100_211136028244_2018040700* | awk '{a+= $0}END{print a}'`

问题4：hadoop输入路径支持的通配符很强
参考[hadoop输入路径读取文件的正则通配符](https://blog.csdn.net/u013013024/article/details/53128071)

问题5：现在的问题是应该是给定数据大小，得到最优的参数，所谓最优的意思就是时间最短。那么这个数据怎么利用呢？
最优化方法比如DFO算法需要真正的求解函数值，一是代价大，二是肯定是数据越小时间短，因此size是一个需要固定的参数，但是他又是一个连续值。还有一个问题就是当前手头数据的不平衡，小数据的执行结果过多，大数据的执行较少。
目前可以考虑的就是首先通过插值算法或ML回归模型拟合时间函数。然后通过DFO局部最优化方法或者粒子群全局最优化方法求最小点。

问题6：配置crontab
尚未解决

问题7：job出现错误`Container killed on request. Exit code is 143`？
可能是由于物理内存达到限制，导致container被kill掉报错。目前通过增加内存到40g的方式可以解决问题。注意增加48g会突破的yarn的memory限制。
* * *




- 日期：201804010
- 工作目标：重构脚本，优化命令行参数
基本完成

- 工作目标：理解各个表的参数
对于PRE表，需要hdfs输入输出路径，然后同时会在hive的pre_table中写数据，命令参数中的省份长码只会影响到任务名。对于子表，实际上配置文件基本没用，然而省份长码会控制写出子表中的内容，这里的上分长吗要注意下。

- 问题：全国的批处理一遍太慢了，这个问题一是不能满足要求，二是影响调参效率。
* * *


- 日期：20180411
- 工作目标：尽快完成AB表数据的输出。
从代码上来看，目前AB表的处理是不区分省份的，因此我感觉有些问题，与崔老师沟通后他表示会尽快修改，并且修改之后会解决小文件太多处理过慢的问题。并表示修改之后会利用省份进行分区，
修改之后PRE的命令行参数中的省份长码会影响向hive中写数据。

- 问题：hdfs上的数据情况
内蒙古和天津9号之后的是纯文本，北京10号之后的是纯文本。

- 问题：崔老师对hive进行了partition，这个不太明白，需要查一下

- 问题：改变了spark的提交方式，从yarn变成了yarn-cluster，这个不太明白，需要查一下

- 问题：读取文件夹内篇日志文件失败

* * *

- 日期：20180412
- 问题：AB表需要测吗？怎么测？

- 问题：hadoop-slave失去连接


* * *

- 日期：20180413
- 任务：小批量数据大量测试
目前进行压缩包数据的大量测试，全部处理一遍基本需要12小时，在这一步骤中遇到问题，选取的十个数据量较小的省份中，有三个省一定会出错，目前还没有排查到原因，处理手段是放弃先放弃这三个省份，将问题提交至崔老师，然后继续跑后续省份。
目标是获得较大量程序运行结果后进行最优参数的搜索，但是目前遇到的主要几个问题是：（1）日后正式上线应该是纯文本的文件，本周发现过一次程序的问题，一次数据的问题，崔老师调整好没有问题的实时数据是本周五开始上传，导致本周没有采集到可以使用的运行时间数据。（2）程序本周内进行过一次改动，改动后程序运行时间与之前运行时间发生变化。（3）一次运行需要的时间太慢太慢，集群资源不足，无法同时并发大量任务，导致时间数据采集效率低下。目前该问题没有解决的较好办法。因此需要调整的参数较多，变动的范围较大，肯定需要大量尝试才可以得到最优参数组合。

- 问题：子表怎么知道自己处理的是哪一天的数据

* * *

- 日期：20180409 - 20180413
- 本周进展：

1. 基本掌握批处理部分全部代码逻辑，参数设置
2. 完成了自动化运行脚本，配置文件生成脚本与数据采集功能如果程序不出错，可以全部自动化完成。
3. 在测试过程中发现代码逻辑问题，与崔华俊老师协调后修复。在测试过程中发现数据问题，与崔华俊老师沟通后修改上传日志。
4. 完成了十个省份的批处理部分的6个表的整体流程与处理数据生成。


问题： 43g超出最大内存了，要小心

* * *

- 日期： 20180419
- 工作目标：初步了解hive工作原理。理解AB表工作流程，解决争吵师兄提出的问题.AB表内数据有误。

- hive 知识点：
参考：

1. [Hive之分区（Partitions）和桶（Buckets）](http://www.aahyhaa.com/archives/316)
2. [hive深入浅出](https://blog.csdn.net/hguisu/article/details/18986759)
3. [Hive原理及查询优化](https://blog.csdn.net/LW_GHY/article/details/51469753)

- AB表遇到的问题1：没有跑19号的数据，但是pre\_table中有19号的分区数据。看的是分区18号的表，但是里面的数据字段显示20180411。
分区键是dt，dt指的是哪天运行，表里面的第一个字段才是真正的数据的日期。

- 即使这样，现在等与是我今天19号，跑15号到18号的数据，dt就是19，但是表里面的字段有15-18？再有就是如果dt作为分区键，我为啥不把前一天跑的数据都删除掉？在AB表读取数据的时候只考虑省份长码，不考虑日期，这样是不是很影响效率？删除前一天表的问题是在代码中解决还是额外写脚本？

- drop table之后底层文件残留？
权限粘滞位（Sticky bit）问题，已解决

* * *

- 日期：20180420
- 问题1：A1表跑完只有几条数据吗？这几条数据里还有重复的应该是代码逻辑问题吧？我是20号对19号的三个省先跑批。为啥结果里面是1718号的数据？
没有问题，因为A1D表功能上实现的就是一天一个省份出一条日志。但是更严重的问题是后面这个，数据积压的问题，按理说理想情况是我跑19号的PRE批，跑了三个省，然后跑A1D，里面应该有三条，分别是三个省19号的统计结果。但是这个问题现在无法解决。崔老师说原因在于现在的日志是从移动一期的FTP迁移过来的，一期的集群复杂，且flume由于性能等问题导致数据积压，这个问题无法解决，但下周从二期的FTP上直接上传日志的时候再看看还有没有问题。

- 问题2：跑A1H时跑安徽的，但是pre表中应该没有安徽的数据啊？
这个问题暂且搁置

* * *

- 日期：20180423
- 问题：继续上面的问题2，尝试分析
症状是：我在20号只跑了19号的数据，先不管19号里面有其他日期的数据，省份我是只跑了三个省的，我看了一下hdfs上原始日志的内容，文件内是没有省份信息的（但是有真实的时间戳），也就是说省份信息靠文件夹定位，那么我跑了三个省份的，pre_table应该也只有三个省份了，我看了一下pre_table里面也的确只有三个省份，但是A1H跑起来的确是会有一些其他的省份出现，这个问题不行就在确认一下，但是之前周五确认过了的确有这个问题。

- 代码更新
崔老师更新一版本代码，改变dt分区模式，dt需要手动以命令行参数输入作为分区键。这样的话需要测试一些新的功能。对于我的自动化脚本来说，需要更改的地方就是remote脚本，真正提交命令的地方，其他两个脚本来维护date变量，并且表示的意思是处理那天的日志，无需改动。然后目前来看A表使用sql取数据都是同时依赖省份码和日期，解决了前面指出的问题。

- [x] 把全部hive中的table删除，然后跑省公司直接上传的数据，手动查看原始日志文件，手动查看生成的pre\_table，看看有没有数据滞后性的问题。（先跑一天的省份的）

- [x] 然后执行A1D，检查生成日期和总条数

- [x] 然后执行A1H，查看数据是否正常

- [x] 如果都没有问题，测试A2

- 工作目标：正则表达式编写
匹配闰年2月29：(([13579][26]|[2468][048])00|\d{2}([13579][26]|[2468][048]|0[48]))0229
匹配正常日期：(?:\d{4}(?:(?:1[02]|0[3578])(?:[12][0-9]|3[01]|0[1-9])|(?:0[469]|11)(?:[12][0-9]|30|0[0-9])|02(?:[1][0-9]|2[0-8]|0[1-9])))
匹配时间：([01]\d|2[0-3])[012345]\d[012345]\d
匹配14位时间戳：(?:(([13579][26]|[2468][048])00|\d{2}([13579][26]|[2468][048]|0[48]))0229|(?:\d{4}(?:(?:1[02]|0[3578])(?:[12][0-9]|3[01]|0[1-9])|(?:0[469]|11)(?:[12][0-9]|30|0[0-9])|02(?:[1][0-9]|2[0-8]|0[1-9]))))([01]\d|2[0-3])[012345]\d[012345]\d

用新的正则表达式匹配原来配置文件中的匹配规则，只需要改pre的就行，子表的配置文件中的相关项目可以删掉了


* * *

- 日期：20180427
- 任务：理解zookeeper

参考：

1. 《大数据日知录：架构与算法》
2. [ZAB协议分析](http://blog.jobbole.com/104985/)
3. [zookeeper CLI](https://www.w3cschool.cn/zookeeper/zookeeper_cli.html)


* * *

- 日期：20180428
- 任务：配置zookeeper，kafka，spark，编写spark streaming



参考资料:
1. [linux time命令详解](http://blog.chinaunix.net/uid-26557245-id-3782974.html)
2. [expect基础语法](https://blog.csdn.net/k122769836/article/details/49051243)
3. [expect用法](https://www.cnblogs.com/tychyg/p/5086499.html)
4. [spark通过合理设置spark.default.parallelism参数提高执行效率](https://blog.csdn.net/bbaiggey/article/details/51984753)
5. [spark 资源参数调优](https://www.cnblogs.com/bonelee/p/6042267.html)
6. [Spark性能优化指南——基础篇](https://tech.meituan.com/spark-tuning-basic.html)
7. [Spark性能优化指南——高级篇](https://tech.meituan.com/spark-tuning-pro.html)
8. [关于sparksql on yarn生成大量.hive-staging文件问题](http://www.aboutyun.com/thread-20657-1-1.html)



