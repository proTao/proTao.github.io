---
layout: post
title: 数据的平滑方法
category: 自然语言处理
tags: nlp
keywords: smooth nlp
description: 
---

## 数据平滑
- 数据平滑的基本思想:
	调整最大似然估计的概率值,使零概率增值,使非零概率下调,“劫富济贫”,消除零概率,改进模型的整体正确率。
- 基本目标:测试样本的语言模型困惑度越小越好。
- 基本约束: \\(\Sigma p ( wi | w_{1}, w_{2},..., w_{i-1})=1\\)

### 拉普拉斯平滑
其实就是最简单的加一平滑。基本思想是在当前语言模型中，每一种可能出现的基元情况都加1。也就是说在原本的最大似然估计的基础上，分子加一，分母加上语言模型词汇表的大小。例如对于2-gram模型来说，进行如下平滑：

![](/img/2gramsmooth.png)

该算法可以把加一改进为加上一个delta，通常是一个小于1的正数，效果会得到少许提升。

###减值法/折扣法(Discount)

#### Good-Turing估计

该方法适用于大词汇集产生的符合多项式分布的大量的观测数据。其基本思想是：利用频率的类别信息来平滑频率。对于任何发生r次数的n元语法，都假设它发生了r*次。

![](/img/goodturing.png)

其中，nr是训练语料中正好发生r次的n-gram的个数。也就是说，发生r次的N元组的调整由发生r次的N元组与发生r+1次的n-gram两个类别共同决定。
经过上述计算后，出现次数大于等于1次的基元的频率降低，导致总的有\\(n_1/N\\)的频率被节省下来，将这些频率给那些未见词使用。

按照公式的话，出现次数最多的频次被平滑后被变成0（因为比这次词出现次数还多的词的数目是零），所以在处理中可以不进行处理这些出现次数最多的词，这样的话有可能导致不归一的情况，最后需要人为归一化。

####Back-Off法

基本思想:当某一事件在样本中出现的频率大于阈值K (通常取 K 为0 或1)时,运用最大似然估计的减值法来估计其概率,否则,使用低阶的,即 (n-1)gram 的概率替代 n-gram 概率,而这种替代需受归一化因子\\(\alpha\\)的作用。
可以这么理解：对于每个计数 r > 0 的n元文法的出现次数减值,把因减值而节省下来的剩余概率根据低阶的 (n-1)gram 分配给未见事件。
计算公式为：
<!-- more -->

![](/img/Katz.png)

其中，,\\(p_{ML}(wi)\\)表示 wi 的最大似然(maximum likelihood)估计概率。这个公式的意思是,所有具有非零计数 r 的 2元语法都根据折扣率dr (0<dr<1)被减值了,折扣率 dr 近似地等于 r*/r,减值由Good-Turing估计方法预测。
而公式里面的\\(\alpha\\)是用于归一化的系数，也就是先计算出其他部分，然后通过调整alpha使得所有的概率和为1.

#### 绝对减值法
基本思想:从每个计数 r 中减去同样的量,剩余的概率量由未见事件均分。

![](/img/absolute.png)

R是所有可能事件的数目，假设字典规模是L，采用n-gram模型，那么R=\\(L^n\\)。b是自由参数。n0是样本中未出现的事件的数目。

#### 线性减值法
基本思想:从每个计数 r 中减去与该计数成正比的量(减值函数为线性的),剩余概率量\\(\alpha\\)被n0个未见事件均分。

![](/img/linearsmooth.png)

实践中alpha较好的设置值为n1/N。尽管这个方法看上去更合理更有说服力，但是实践中的结果是绝对减值法的效果要优于线性减值法。

#### 四种减值法的比较
- Good-Turing 法:对非0事件按公式削减出现的次数,节留出来的概率均分给0概率事件。
- Katz 后退法:对非0事件按Good-Turing法计算减值,节留出来的概率按低阶分布分给0概率事件。
- 绝对减值法:对非0事件无条件削减某一固定的出现次数值,节留出来的概率均分给0概率事件。
- 线性减值法:对非0事件根据出现次数按比例削减次数值,节留出来的概率均分给0概率事件。

### 删除插值法
基本思想:用低阶语法估计高阶语法,即当 3-gram的值不能从训练数据中准确估计时,用 2-gram 来替代,同样,当 2-gram 的值不能从训练语料中准确估计时,可以用 1-gram 的值来代替。插值公式为:

![](/img/interpolation.png)

其中要满足：\\(\lambda_1+\lambda_2+\lambda_3 = 1\\)。
确定参数的方法通常是：将训练语料分为两部分,即从原始语料中删除一部分作为留存数据(heldout data)。第一部分用于估计 p '( w 3 | w 1w 2 ) , p '( w 3 | w 2 ) 和 p '( w 3 ) 。第二部分用于计算\\(\lambda_1，\lambda_2，\lambda_3\\)使语言模型对留存数据的困惑度最小。

