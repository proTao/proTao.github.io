---
layout: post
title: 信息论基础
category: 数学
tags: 
- maths
- bigdata
- machinelearning
keywords: 
description: 
---

## 信息论基础

### 熵
![](/img/entropy.png)
如上图在以2为底计算的时候，信息熵的单位是比特，如果以e为底则是奈特，以3为底则是铁特。
信息熵的意义是一个离散随机变量的不确定度，换言之也就是获取到一位这样的随机变量所得到的信息的大小。所以当一个有n种可能性的离散随机变量均匀分布，各个符号的可能性都是1/n时，信息上取最大值log(n)

几种常见语言字母的信息熵如下：

|语言|熵|
|----|----|
|法语|3.98|
|意大利语|4.00|
|西班牙语|4.01|
|英语|4.03|
|俄语|4.35|
以上来自于冯志伟教授的统计结果，20世纪末期，冯志伟教授/刘源教授等人开展对汉语字的信息熵的测定，得到一个汉字的信息熵约为9.7个比特。

### 联合熵
![](/img/joinentropy.png)

联合上的意义是描述一对随机变量平均需要的信息量。

### 条件熵
![](/img/conditionentropy.png)

可以看出条件上实际上是logp(y|x)在联合分布下的期望，条件上反应的是一个条件变量在领一个条件变量给定的情况下的平均不确定性。

另外，上面三个熵的类型有如下关系：
> H(X,Y)=H(X)+H(Y|X)

### 相对熵
![](/img/relativeentropy.png)
相对熵也称KL散度。相对熵可应用来反映两个随机分布之间的差距，相对熵的大小与分布之间的差异呈现正相关，当两个分布之间没有差异的时候，相对熵等于0.

### 交叉熵
![](/img/crossentropy.png)
交叉熵用来衡量估计模型q(x)与真实概率分布p(x)之间的差异。

### 困惑度
![](/img/perplexity.png)
语言模型设计的任务就是寻找困惑度最小的模型,使其最接近真实的语言。

### 互信息
![](/img/mutualinfo.png)
互信息I(X;Y)反映的是互信息 I (X; Y) 是在知道了 Y 的值以后 X 的不确定性的减少量,即Y的值透露了多少关于X的信息量。

互信息/条件熵和联合熵的关系为：
![](/img/1.png)
通过这张图可以看出来为什么熵又称自信息,另一方面说明了两个完全相互依赖的变量之间的互信息并不是一个常量,而是取决于它们的熵。
当两个汉字 x 和 y 关联度较强时,其互信息值I(x, y)>0;x 与y 关系弱时,I(x, y)≈0;而当I(x, y)&lt;0时,x 与 y 称为 “互补分布”。在汉语分词研究中,有学者用双字耦合度的概念代替互信息:设 ci,ci+1是两个连续出现的汉字,统计样本中ci,ci+1连续出现在一个词中的次数和连续出现的总次数,二者之比就是双字耦合度。
在判断两个连续汉字之间的结合强度方面,双字耦合度要比互信息更合适一些。
