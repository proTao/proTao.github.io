---
layout: post
title: 统计机器学习要点总结
date: 2018-08-17
category: 机器学习
tags:
- model
- machinelearning
- algorithm
keywords:
description:
---


## SVM
1. 支持向量机不提供概率输出,而是对新的输入进行分类决策。
2. 需要特征规格化。
3. 线性可分情况下，最优解可证唯一。《统计学习方法》P100。
4. 间隔长度是w模长的倒数的2倍。
5. 最终的解只依赖支撑向量。
6. 惩罚系数C越大，SVM越倾向于将每个点都分类正确；C越小，SVM泛化能力越强，能容忍更多的错误点。
7. 结构风险最小化来提高泛化能力。
8. 非概率模型

## Perceptron
1. 判别模型，目标是找到一个超平面将特征空间中的点分开，决策函数与SVM相同。是神经网络和SVM的基础
2. 学习策略：假设训练集是线性可分的，学习目标是求得一个能够将训练集正负类完全区分的的分离超平面，为了找的这样的超平面。定义**经验损失函数**并极小化损失函数。
3. 损失函数：直观的选择是误分类点的个数，但是该函数难以优化，所以选择修改该损失为误分类点到超平面的距离和，此时损失函数是超平面参数w和b的连续可导函数。
4. 训练算法：随机梯度下降。（不是使得所有的误分类点梯度下降，而是随机选择一个误分类点使其梯度下降。）
5. 收敛性证明：Novikoff定理。（具体证明略过）当训练数据集线性可分的时候，感知机学习算法是收敛的，感知机算法在训练数据集上的误分类次数k存在上界$$$(\frac{max||x||}{\gamma})^2$$$。
6. 线性可分的数据集上，perceptron存在无数多个解，由于不同的初值或者迭代顺序导致不同。
7. 学习算法的对偶形式
8. 优点：计算和概念都十分简单；因为是随机梯度下降，每次选择一个点，所以是常数的内存消耗，并且可以在线学习（在线学习使得可以扩展至大规模数据）。
9. 缺点：非线性可分数据不能收敛；准确度不够；模型过时。
10. 非概率模型

## KNN
1. 是一种基本的分类与回归方法。k值的选择，距离度量以及分类决策规则是k近邻法的三个要素。
2. KD-Tree

## NB
1. 生成模型

## Decision Tree
1. [CART](cart.jpg)
2. 先剪枝：
- 树到达一定高度
- 节点下包含的样本点小于一定数目
- 信息增益小于一定的阈值等等
- 节点下所有样本都属于同一个类别
3. 后剪枝方法：
- 降低错误剪枝 REP(Reduced Error Pruning)
- 悲观错误剪枝 PEP(Pessimistic Error Pruning)
- 基于错误剪枝 EBP(Error Based Pruning)
- 代价-复杂度剪枝 CCP(Cost Complexity Pruning)
- 最小错误剪枝 MEP(Minimum Error Pruning)
CART用的就是CCP剪枝,其余的剪枝方法可以网上google一下. CCP剪枝类实际上就是我们之前讲到的最小化结构风险。结构风险中的正则项在决策树中是叶节点数目。
4. 决策树缺点：不支持在线学习,有新样本来的时候,需要重建决策树.
5. C4.5支持缺失值处理：
- 赋上该属性最常见的值
- 根据节点的样例上该属性值出现的情况赋一个概率,比如该节点上有10个样本,其中属性A的取值有6个为是,4个为否.那么对改节点上缺失的属性A,以0.6的概率设为是,0.4的概率设为否
- 丢弃有缺失值的样本
6. C4.5支持连续值属性：把需要处理的样本(对应根节点)或样本子集(对应子树)按照连续变量的大小从小到大进行排序.假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点,根据这个分割点把原来连续的属性一分为二.但实际上可以不用检查所有N−1个分割点,如果相邻两个值对应的类别一样，那就不用尝试该分割点了，当然回归问题就要挨个尝试了。


## LR

1. LR与朴素贝叶斯的联系
- 相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)P(Y=1|X)形式跟逻辑回归是一样的。
- 不同的地方在于，逻辑回归为判别模型求的是p(y|x)p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。

2. LR与最大熵模型
逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。
- 指数簇分布的最大熵等价于其指数形式的最大似然。
- 二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；
- 多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。

3. LR和SVM对比
- Linear SVM和LR都是线性分类器
- Linear SVM和LR都是判别模型
- Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别处于极其不平衡的状态, 一般需要先对数据做平衡处理。
- Linear SVM依赖数据表达的距离测度，所以需要对数据先做标准化；LR不受其影响
- Linear SVM依赖惩罚项的系数，实验中需要做交叉验证
- Linear SVM和LR的执行都会受到异常值的影响，其敏感程度而言，谁更好很难下明确结论。
- Linear SVM和LR损失函数不同, LR为logloss, SVM为hinge loss. 而SVM中的称为hinge loss。

## RF
1. 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合.
2. 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力.
3. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化
4. 可生成一个 $$$Proximities=(p_{ij})$$$ 矩阵，用于度量样本之间的相似性： $$$p_{ij}=a_{ij}/N$$$, $$$a_{ij}$$$表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数
5. 在创建随机森林的时候，对generlization error使用的是无偏估计
6. 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量。另外，为了尽可能增加子模型受随机性扰动的可能性，每棵树要尽可能的生长，也就是没有剪枝过程，这也使得实现很简单。
7. 在训练过程中，能够检测到feature间的互相影响
8. 容易做成并行化方法。
9. 使用带外估计泛化能力。


### bias-variance分解

![](point2.png)
在bagging和boosting框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging和boosting的基模型都是线性组成的，那么有：
![](point3.png)

对于bagging来说，每个基模型的权重等于1/m且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到：
![](point4.png)

根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。

Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。

对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为：
![](point5.png)

通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。

因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。

基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。


## GBDT
**基分类器的选择**：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。

**二阶泰勒展开**：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。

**方差-方差权衡**：XGBoost在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数T、每个叶子节点上输出分数的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。

**Shrinkage（缩减）**：相当于学习速率（xgboost中的ϵ）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
![](point1.png)
**列抽样（column subsampling）**：XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。
缺失值处理：XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。即对于特征的值有缺失的样本，XGBoost可以自动学习出它的分裂方向。
**XGBoost工具支持并行**：Boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的损失函数里包含了前面t−1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block(块)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。
**线程缓冲区存储**：按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer（缓冲区），主要是结合多线程、数据压缩、分片的方法，然后再计算，提高算法的效率。
**可并行的近似直方图算法**：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。

## Kmeans
- 优点：
    1. 是解决聚类问题的一种经典算法，简单、快速
    2. 对处理大数据集，该算法保持可伸缩性和高效率
    3. 当簇近似为高斯分布时，它的效果较好
- 缺点
    1. 在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用
    2. 必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同
    3. 只能发现球状Cluster，不适合于发现非凸形状的簇或者大小差别很大的簇
    4. 对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重。因为均值体现的是数据集的整体特征，容易掩盖数据本身的特性。

<<<<<<< HEAD:source/_posts/MachingLearning/2018-08-17-point.md
## Optimization
![](point6.png)
![](point7.png)
SGD的变种主要围绕3个方向展开

1. 优化learn_rate → 自适应学习率
    - Annealing 全局共享learn\_rate 所有的参数以相同的幅度进行更新
         - 随步数衰减
         - 指数衰减
         - 1/t衰减
    - Adagrad 参数独立learn\_rate 更新幅度取决于参数本身

2. 优化梯度方向 → 方向感，减小震荡
	- 动量Momentum，Nesterov
	- SAG/SVRG/...

3. 并行化 → Scalable
![](point8.png)




## 参考
1. [机器学习五要素 optimization](https://www.cwzj.com/archives/2602)
2. [](http://www.cnblogs.com/neopenx/p/4768388.html)
3. [](https://zhuanlan.zhihu.com/p/21798784)
4. [](https://zhuanlan.zhihu.com/p/21539419)
5. [](http://www.datakit.cn/blog/2016/07/04/sgd_01.html)
=======
## DBSCAN
密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。除了DBSCAN算法，密度最大值算法也是密度聚类算法的代表算法。

- 优点：
	1. 无需确定聚类个数：DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means.
	2. 可以发现任意形状的聚类：DBSCAN can find arbitrarily shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.
	3. 对噪声具有鲁棒性，可有效处理噪声：DBSCAN has a notion of noise, and is robust to outliers.
	4. 只需两个参数，对数据输入顺序不敏感：DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.)
	5. 加快区查询：DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R\* tree.
	6. 参数可由领域专家设置：The parameters minPts and ε can be set by a domain expert, if the data is well understood.
- 缺点
    1. 边界点不完全确定性：DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data is processed. Fortunately, this situation does not arise often, and has little impact on the clustering result[citation needed]: both on core points and noise points, DBSCAN is deterministic. DBSCAN\*[4] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components.
    2. 维数灾导致欧几里得距离度量失效：The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called “Curse of dimensionality”, making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance.
    3. 不能处理密度差异过大（密度不均匀）的聚类（会导致参数无法适用于所有聚类）：DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters.
    4. 参数选择在数据与规模不能很好理解的情况下，很难选择，若选取不当，聚类质量下降： If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult.


## Optimization
![](../img/point_op0.png)
![](../img/point_op1.gif)
![](../img/point_op2.gif)
![](../img/point_op3.gif)



## VC Dimension
1943年，模拟神经网络由麦卡洛可（McCulloch）和皮茨（Pitts)提出，他们分析了理想化的人工神经元网络，并且指出了它们进行简单逻辑运算的机制。

1957年，康奈尔大学的实验心理学家弗兰克·罗森布拉特(Rosenblatt)在一台IBM–704计算机上模拟实现了一种他发明的叫作“感知机”（Perceptron）的神经网络模型。神经网络与支持向量机都源自于感知机（Perceptron）。

1962年，罗森布拉特著作：《神经动力学原理：感知机和大脑机制的理论》（Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms）。

1969年，明斯基和麻省理工学院的另一位教授佩普特合作著作：《感知机：计算几何学》（Perceptrons: An Introduction to Computational Geometry)。在书中，明斯基和佩普特证明单层神经网络不能解决XOR（异或）问题。

1971年，V. Vapnik and A. Chervonenkis在论文“On the uniform convergence of relative frequencies of events to their probabilities”中提出VC维的概念。

1974年，V. Vapnik提出了结构风险最小化原则。

1974年，沃波斯（Werbos）的博士论文证明了在神经网络多加一层，并且利用“后向传播”（Back-propagation）学习方法，可以解决XOR问题。那时正是神经网络研究的低谷，文章不合时宜。

1982年，在加州理工担任生物物理教授的霍普菲尔德，提出了一种新的神经网络，可以解决一大类模式识别问题，还可以给出一类组合优化问题的近似解。这种神经网络模型后被称为霍普菲尔德网络。

1986年，Rummelhart与McClelland发明了神经网络的学习算法Back Propagation。

1993年，Corinna Cortes和Vapnik等人提出了支持向量机(support vector machine)。神经网络是多层的非线性模型，支持向量机利用核技巧把非线性问题转换成线性问题。

1992~2005年，SVM与Neural network之争，但被互联网风潮掩盖住了。

2006年，Hinton提出神经网络的Deep Learning算法。Deep Learning假设神经网络是多层的，首先用Restricted Boltzmann Machine（非监督学习）学习网络的结构，然后再通过Back Propagation（监督学习）学习网络的权值。

现在，deep learning的应用越来越广泛，甚至已经有超越SVM的趋势。一方面以Hinton，Lecun为首的深度学习派坚信其有效实用性，另一方面Vapnik等统计机器学习理论专家又坚持着理论阵地，怀疑deep learning的泛化界。

## 可能问到的面试问题
### 解决过拟合
1. 特征选择或者降维
2. 正则化
3. 树：剪枝
4. 深度学习：Dropout、BN
>>>>>>> b0832de437b3d4c1cda2a32939e3028e6558d931:source/_drafts/point.md
