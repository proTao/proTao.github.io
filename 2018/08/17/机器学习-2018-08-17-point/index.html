<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>统计机器学习要点总结 | proTao的大脑具现</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="注：仅用于个人复习1. SVM 支持向量机不提供概率输出,而是对新的输入进行分类决策。 需要特征规格化。 线性可分情况下，最优解可证唯一。《统计学习方法》P100。 间隔长度是w模长的倒数的2倍。 最终的解只依赖支撑向量。 惩罚系数C越大，SVM越倾向于将每个点都分类正确；C越小，SVM泛化能力越强，能容忍更多的错误点。 结构风险最小化来提高泛化能力。 变量的选择方法：第一个变量的选择是外层循环，">
<meta name="keywords" content="machinelearning,algorithm,model">
<meta property="og:type" content="article">
<meta property="og:title" content="统计机器学习要点总结">
<meta property="og:url" content="https://protao.github.io/2018/08/17/机器学习-2018-08-17-point/index.html">
<meta property="og:site_name" content="proTao的大脑具现">
<meta property="og:description" content="注：仅用于个人复习1. SVM 支持向量机不提供概率输出,而是对新的输入进行分类决策。 需要特征规格化。 线性可分情况下，最优解可证唯一。《统计学习方法》P100。 间隔长度是w模长的倒数的2倍。 最终的解只依赖支撑向量。 惩罚系数C越大，SVM越倾向于将每个点都分类正确；C越小，SVM泛化能力越强，能容忍更多的错误点。 结构风险最小化来提高泛化能力。 变量的选择方法：第一个变量的选择是外层循环，">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://protao.github.io/img/point2.png">
<meta property="og:image" content="https://protao.github.io/img/point3.png">
<meta property="og:image" content="https://protao.github.io/img/point4.png">
<meta property="og:image" content="https://protao.github.io/img/point5.png">
<meta property="og:image" content="https://protao.github.io/img/point1.png">
<meta property="og:image" content="https://protao.github.io/img/point6.png">
<meta property="og:image" content="https://protao.github.io/img/point7.png">
<meta property="og:image" content="https://protao.github.io/img/point8.png">
<meta property="og:image" content="https://protao.github.io/img/point_op0.png">
<meta property="og:image" content="https://protao.github.io/img/point_op1.gif">
<meta property="og:image" content="https://protao.github.io/img/point_op2.gif">
<meta property="og:image" content="https://protao.github.io/img/point_op3.gif">
<meta property="og:updated_time" content="2019-05-11T07:05:53.405Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="统计机器学习要点总结">
<meta name="twitter:description" content="注：仅用于个人复习1. SVM 支持向量机不提供概率输出,而是对新的输入进行分类决策。 需要特征规格化。 线性可分情况下，最优解可证唯一。《统计学习方法》P100。 间隔长度是w模长的倒数的2倍。 最终的解只依赖支撑向量。 惩罚系数C越大，SVM越倾向于将每个点都分类正确；C越小，SVM泛化能力越强，能容忍更多的错误点。 结构风险最小化来提高泛化能力。 变量的选择方法：第一个变量的选择是外层循环，">
<meta name="twitter:image" content="https://protao.github.io/img/point2.png">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="../../../../css/style.css">
  <meta name="baidu-site-verification" content="xerEdoxBbf">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['\\(','\\)'],['$$$','$$$'],['$','$']]}
});
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="../../../../index.html" id="logo">proTao的大脑具现</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="../../../../index.html">home</a>
        
          <a class="main-nav-link" href="../../../../archives">archives</a>
        
          <a class="main-nav-link" href="../../../../about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://protao.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">

<article id="post-机器学习-2018-08-17-point" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="" class="article-date">
  <time datetime="2018-08-16T16:00:00.000Z" itemprop="datePublished">2018-08-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="../../../../categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      统计机器学习要点总结
    </h1>
  
 
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="注：仅用于个人复习"><a href="#注：仅用于个人复习" class="headerlink" title="注：仅用于个人复习"></a>注：仅用于个人复习</h1><h2 id="1-SVM"><a href="#1-SVM" class="headerlink" title="1. SVM"></a>1. SVM</h2><ol>
<li>支持向量机不提供概率输出,而是对新的输入进行分类决策。</li>
<li>需要特征规格化。</li>
<li>线性可分情况下，最优解可证唯一。《统计学习方法》P100。</li>
<li>间隔长度是w模长的倒数的2倍。</li>
<li>最终的解只依赖支撑向量。</li>
<li>惩罚系数C越大，SVM越倾向于将每个点都分类正确；C越小，SVM泛化能力越强，能容忍更多的错误点。</li>
<li>结构风险最小化来提高泛化能力。</li>
<li>变量的选择方法：第一个变量的选择是外层循环，原则是违反KKT条件最严重的样本点，这里的KKT条件就是拉格朗日系数要和其距离决策面的距离一致。优先检验恰好落在间隔上的点。第二个变量的选择标准是希望被选择的变量可以有足够大的变化。如果不能是目标函数下降的足够多，会重新去选择第一个变量。</li>
</ol>
<a id="more"></a>
<h2 id="2-LR"><a href="#2-LR" class="headerlink" title="2. LR"></a>2. LR</h2><ol>
<li>LR与朴素贝叶斯的联系</li>
</ol>
<ul>
<li>相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率P(X|Y=ck)P(X|Y=ck)服从高斯分布时Gaussian Naive Bayes，它计算出来的P(Y=1|X)P(Y=1|X)形式跟逻辑回归是一样的。</li>
<li>不同的地方在于，逻辑回归为判别模型求的是p(y|x)p(y|x)，朴素贝叶斯为生成模型求的是p(x,y)p(x,y)。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率P(X|Y=ck)P(X|Y=ck)是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。</li>
</ul>
<ol start="2">
<li>LR与最大熵模型<br>逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。</li>
</ol>
<ul>
<li>指数簇分布的最大熵等价于其指数形式的最大似然。</li>
<li>二项式分布的最大熵解等价于二项式指数形式(sigmoid)的最大似然；</li>
<li>多项式分布的最大熵等价于多项式分布指数形式(softmax)的最大似然。</li>
</ul>
<ol start="3">
<li>LR和SVM对比</li>
</ol>
<ul>
<li>Linear SVM和LR都是线性分类器</li>
<li>Linear SVM和LR都是判别模型</li>
<li>Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别处于极其不平衡的状态, 一般需要先对数据做平衡处理。</li>
<li>Linear SVM依赖数据表达的距离测度，所以需要对数据先做标准化；LR不受其影响</li>
<li>Linear SVM依赖惩罚项的系数，实验中需要做交叉验证</li>
<li>Linear SVM和LR的执行都会受到异常值的影响，其敏感程度而言，谁更好很难下明确结论。</li>
<li>Linear SVM和LR损失函数不同, LR为logloss, SVM为hinge loss。</li>
</ul>
<h2 id="3-Perceptron"><a href="#3-Perceptron" class="headerlink" title="3. Perceptron"></a>3. Perceptron</h2><ol>
<li>判别模型，目标是找到一个超平面将特征空间中的点分开，决策函数与SVM相同。是神经网络和SVM的基础</li>
<li>学习策略：假设训练集是线性可分的，学习目标是求得一个能够将训练集正负类完全区分的的分离超平面，为了找的这样的超平面。定义<strong>经验损失函数</strong>并极小化损失函数。</li>
<li>损失函数：直观的选择是误分类点的个数，但是该函数难以优化，所以选择修改该损失为误分类点到超平面的距离和，此时损失函数是超平面参数w和b的连续可导函数。</li>
<li>训练算法：随机梯度下降。（不是使得所有的误分类点梯度下降，而是随机选择一个误分类点使其梯度下降。）</li>
<li>收敛性证明：Novikoff定理。当训练数据集线性可分的时候，感知机学习算法是收敛的，感知机算法在训练数据集上的误分类次数k存在上界$$$(\frac{max||x||}{\gamma})^2$$$。</li>
<li>线性可分的数据集上，perceptron存在无数多个解，由于不同的初值或者迭代顺序导致不同。</li>
<li>学习算法的对偶形式</li>
<li>优点：计算和概念都十分简单；因为是随机梯度下降，每次选择一个点，所以是常数的内存消耗，并且可以在线学习（在线学习使得可以扩展至大规模数据）。</li>
<li>缺点：非线性可分数据不能收敛；准确度不够；模型过时。</li>
<li>非概率模型</li>
</ol>
<h2 id="4-KNN"><a href="#4-KNN" class="headerlink" title="4. KNN"></a>4. KNN</h2><ol>
<li>是一种基本的分类与回归方法。k值的选择，距离度量以及分类决策规则是k近邻法的三个要素。</li>
<li>KD-Tree</li>
</ol>
<h2 id="5-Decision-Tree"><a href="#5-Decision-Tree" class="headerlink" title="5. Decision Tree"></a>5. Decision Tree</h2><ol>
<li><a href="cart.jpg">CART</a></li>
<li>先剪枝：</li>
</ol>
<ul>
<li>树到达一定高度</li>
<li>节点下包含的样本点小于一定数目</li>
<li>信息增益小于一定的阈值等等</li>
<li>节点下所有样本都属于同一个类别</li>
</ul>
<ol start="3">
<li>后剪枝方法：</li>
</ol>
<ul>
<li>降低错误剪枝 REP(Reduced Error Pruning)</li>
<li>悲观错误剪枝 PEP(Pessimistic Error Pruning)</li>
<li>基于错误剪枝 EBP(Error Based Pruning)</li>
<li>代价-复杂度剪枝 CCP(Cost Complexity Pruning)</li>
<li>最小错误剪枝 MEP(Minimum Error Pruning)<br>CART用的就是CCP剪枝,其余的剪枝方法可以网上google一下. CCP剪枝类实际上就是我们之前讲到的最小化结构风险。结构风险中的正则项在决策树中是叶节点数目。</li>
</ul>
<ol start="4">
<li>决策树缺点：不支持在线学习,有新样本来的时候,需要重建决策树.</li>
<li>C4.5支持缺失值处理：</li>
</ol>
<ul>
<li>赋上该属性最常见的值</li>
<li>根据节点的样例上该属性值出现的情况赋一个概率,比如该节点上有10个样本,其中属性A的取值有6个为是,4个为否.那么对改节点上缺失的属性A,以0.6的概率设为是,0.4的概率设为否</li>
<li>丢弃有缺失值的样本</li>
</ul>
<ol start="6">
<li>C4.5支持连续值属性：把需要处理的样本(对应根节点)或样本子集(对应子树)按照连续变量的大小从小到大进行排序.假设该属性对应的不同的属性值一共有N个,那么总共有N−1个可能的候选分割阈值点,每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点,根据这个分割点把原来连续的属性一分为二.但实际上可以不用检查所有N−1个分割点,如果相邻两个值对应的类别一样，那就不用尝试该分割点了，当然回归问题就要挨个尝试了。</li>
</ol>
<h2 id="5-RF"><a href="#5-RF" class="headerlink" title="5. RF"></a>5. RF</h2><ol>
<li>在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合.</li>
<li>在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力.</li>
<li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li>
<li>可生成一个 $$$Proximities=(p_{ij})$$$ 矩阵，用于度量样本之间的相似性： $$$p_{ij}=a_{ij}/N$$$, $$$a_{ij}$$$表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数</li>
<li>在创建随机森林的时候，对generlization error使用的是无偏估计</li>
<li>训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量。另外，为了尽可能增加子模型受随机性扰动的可能性，每棵树要尽可能的生长，也就是没有剪枝过程，这也使得实现很简单。</li>
<li>在训练过程中，能够检测到feature间的互相影响</li>
<li>容易做成并行化方法。</li>
<li>使用包外估计泛化能力。</li>
<li>特征对目标变量预测的相对重要性可以通过（树中的决策节点的）特征使用的相对顺序（即深度）来进行评估。 决策树顶部使用的特征对更大一部分输入样本的最终预测决策做出贡献；因此，可以使用接受每个特征对最终预测的贡献的样本比例来评估该 特征的相对重要性 。单个决策树本质上是通过选择最佳切分点来进行特征选择.这个信息可以用来评定每个特征的重要性。基本思想是：在树的分割点中使用的特征越频繁，特征越重要。 这个特征重要性的概念可以通过简单地平均每棵树的特征重要性来扩展到决策树集合。</li>
</ol>
<p>通过对多个随机树中的 预期贡献率 （expected activity rates） 取平均，可以减少这种估计的 方差 ，并将其用于特征选择。</p>
<h3 id="bias-variance分解"><a href="#bias-variance分解" class="headerlink" title="bias-variance分解"></a>bias-variance分解</h3><p><img src="/img/point2.png" alt><br>在bagging和boosting框架中，通过计算基模型的期望和方差，我们可以得到模型整体的期望和方差。为了简化模型，我们假设基模型的权重、方差及两两间的相关系数相等。由于bagging和boosting的基模型都是线性组成的，那么有：<br><img src="/img/point3.png" alt></p>
<p>对于bagging来说，每个基模型的权重等于1/m且期望近似相等（子训练集都是从原训练集中进行子抽样），故我们可以进一步化简得到：<br><img src="/img/point4.png" alt></p>
<p>根据上式我们可以看到，整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，在此我们还知道了为什么bagging中的基模型一定要为强模型，否则就会导致整体模型的偏差度低，即准确度低。</p>
<p>Random Forest是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。Random Fores中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。</p>
<p>对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为：<br><img src="/img/point5.png" alt></p>
<p>通过观察整体方差的表达式，我们容易发现，若基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，boosting框架中的基模型必须为弱模型。</p>
<p>因为基模型为弱模型，导致了每个基模型的准确度都不是很高（因为其在训练集上的准确度不高）。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的准确度提高，而随着训练的进行，整体模型的方差变大，导致防止过拟合的能力变弱，最终导致了准确度反而有所下降。</p>
<p>基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同Random Forrest，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。</p>
<h2 id="6-GBDT、XGBoost-与LightGBM"><a href="#6-GBDT、XGBoost-与LightGBM" class="headerlink" title="6. GBDT、XGBoost 与LightGBM"></a>6. GBDT、XGBoost 与LightGBM</h2><p><strong>基分类器的选择</strong>：传统GBDT以CART作为基分类器，XGBoost还支持线性分类器，这个时候XGBoost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</p>
<p><strong>二阶泰勒展开</strong>：传统GBDT在优化时只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，XGBoost工具支持自定义损失函数，只要函数可一阶和二阶求导。</p>
<p><strong>方差-方差权衡</strong>：XGBoost在目标函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数T、每个叶子节点上输出分数的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性。</p>
<p><strong>Shrinkage（缩减）</strong>：相当于学习速率（xgboost中的ϵ）。XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）<br><img src="/img/point1.png" alt><br><strong>列抽样（column subsampling）</strong>：XGBoost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是XGBoost异于传统GBDT的一个特性。<br>缺失值处理：XGBoost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。即对于特征的值有缺失的样本，XGBoost可以自动学习出它的分裂方向。<br><strong>XGBoost工具支持并行</strong>：Boosting不是一种串行的结构吗?怎么并行的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的损失函数里包含了前面t−1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block(块)结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。<br><strong>线程缓冲区存储</strong>：按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer（缓冲区），主要是结合多线程、数据压缩、分片的方法，然后再计算，提高算法的效率。<br><strong>可并行的近似直方图算法</strong>：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。</p>
<h3 id="补充：LightGBM"><a href="#补充：LightGBM" class="headerlink" title="补充：LightGBM"></a>补充：LightGBM</h3><ul>
<li>基于Histogram的决策树算法</li>
<li>带深度限制的Leaf-wise的叶子生长策略</li>
<li>直方图做差加速</li>
<li>直接支持类别特征</li>
<li>Cache命中率优化</li>
<li>基于直方图的稀疏特征优化</li>
<li>多线程优化</li>
</ul>
<h2 id="7-Kmeans"><a href="#7-Kmeans" class="headerlink" title="7. Kmeans"></a>7. Kmeans</h2><h3 id="7-1-知识点"><a href="#7-1-知识点" class="headerlink" title="7.1 知识点"></a>7.1 知识点</h3><ul>
<li>优点：<ol>
<li>是解决聚类问题的一种经典算法，简单、快速</li>
<li>对处理大数据集，该算法保持可伸缩性和高效率，计算复杂度是O(NKt)</li>
<li>当簇近似为高斯分布时，它的效果较好</li>
</ol>
</li>
<li>缺点<ol>
<li>在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用</li>
<li>必须事先给出K，而且对初值敏感，对于不同的初始值，结果可能不同</li>
<li>只能发现球状Cluster，不适合于发现非凸形状的簇或者<strong>大小差别很大的簇</strong>。</li>
<li>对噪声和孤立点数据敏感，如簇中含有异常点，将导致均值偏离严重。因为均值体现的是数据集的整体特征，容易掩盖数据本身的特性。</li>
</ol>
</li>
</ul>
<h3 id="7-2-更好的选取K的方式：Gap-Statistic"><a href="#7-2-更好的选取K的方式：Gap-Statistic" class="headerlink" title="7.2 更好的选取K的方式：Gap-Statistic"></a>7.2 更好的选取K的方式：Gap-Statistic</h3><p>$$Gap(K) = E(logD_K)-logD_K$$<br>这里的$$$D_K$$$就是指算法的损失函数，也就是平均距离，这个期望怎么算呢，就是在样本所在的范围内按照均匀分布随机产生样本点，然后对这个新的模拟的均匀样本运行kmeans算法，通常来说，对于这样的样本，算法的效果会很差，所以损失会比较大，而如果kmeans的效果比较好，的话，就会使得这个Gap值较大。</p>
<h3 id="7-3-改进模型一：Kmeans"><a href="#7-3-改进模型一：Kmeans" class="headerlink" title="7.3 改进模型一：Kmeans++"></a>7.3 改进模型一：Kmeans++</h3><p>这个算法改进的点是对初始值的选择。原始的Kmeans是随机选择k个点，随机选择一个点作为第一个中心，然后当选择了n个聚类中心的时候，距离当前n个中心越远的点越有更高的概率成为新的中心。</p>
<h3 id="7-4-改进模型一：ISODATA"><a href="#7-4-改进模型一：ISODATA" class="headerlink" title="7.4 改进模型一：ISODATA"></a>7.4 改进模型一：ISODATA</h3><p>改进的点是动态的选择K。增加两个操作：分裂和合并。需要参数：预期的K，每个类的最少样本数目，最大类内方差，最小聚类中心距离。</p>
<h3 id="7-5-与GMM和EM算法的关系"><a href="#7-5-与GMM和EM算法的关系" class="headerlink" title="7.5 与GMM和EM算法的关系"></a>7.5 与GMM和EM算法的关系</h3><h3 id="7-6-聚类的评估方法"><a href="#7-6-聚类的评估方法" class="headerlink" title="7.6 聚类的评估方法"></a>7.6 聚类的评估方法</h3><h3 id="7-7-其他聚类方法：DBSCAN"><a href="#7-7-其他聚类方法：DBSCAN" class="headerlink" title="7.7 其他聚类方法：DBSCAN"></a>7.7 其他聚类方法：DBSCAN</h3><p>密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。除了DBSCAN算法，密度最大值算法也是密度聚类算法的代表算法。</p>
<ul>
<li>优点：<ol>
<li>无需确定聚类个数：DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means.</li>
<li>可以发现任意形状的聚类：DBSCAN can find arbitrarily shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster. Due to the MinPts parameter, the so-called single-link effect (different clusters being connected by a thin line of points) is reduced.</li>
<li>对噪声具有鲁棒性，可有效处理噪声：DBSCAN has a notion of noise, and is robust to outliers.</li>
<li>只需两个参数，对数据输入顺序不敏感：DBSCAN requires just two parameters and is mostly insensitive to the ordering of the points in the database. (However, points sitting on the edge of two different clusters might swap cluster membership if the ordering of the points is changed, and the cluster assignment is unique only up to isomorphism.)</li>
<li>加快区查询：DBSCAN is designed for use with databases that can accelerate region queries, e.g. using an R* tree.</li>
<li>参数可由领域专家设置：The parameters minPts and ε can be set by a domain expert, if the data is well understood.</li>
</ol>
</li>
<li>缺点<ol>
<li>边界点不完全确定性：DBSCAN is not entirely deterministic: border points that are reachable from more than one cluster can be part of either cluster, depending on the order the data is processed. Fortunately, this situation does not arise often, and has little impact on the clustering result[citation needed]: both on core points and noise points, DBSCAN is deterministic. DBSCAN*[4] is a variation that treats border points as noise, and this way achieves a fully deterministic result as well as a more consistent statistical interpretation of density-connected components.</li>
<li>维数灾导致欧几里得距离度量失效：The quality of DBSCAN depends on the distance measure used in the function regionQuery(P,ε). The most common distance metric used is Euclidean distance. Especially for high-dimensional data, this metric can be rendered almost useless due to the so-called “Curse of dimensionality”, making it difficult to find an appropriate value for ε. This effect, however, is also present in any other algorithm based on Euclidean distance.</li>
<li>不能处理密度差异过大（密度不均匀）的聚类（会导致参数无法适用于所有聚类）：DBSCAN cannot cluster data sets well with large differences in densities, since the minPts-ε combination cannot then be chosen appropriately for all clusters.</li>
<li>参数选择在数据与规模不能很好理解的情况下，很难选择，若选取不当，聚类质量下降： If the data and scale are not well understood, choosing a meaningful distance threshold ε can be difficult.</li>
</ol>
</li>
</ul>
<h2 id="8-Optimization"><a href="#8-Optimization" class="headerlink" title="8. Optimization"></a>8. Optimization</h2><p><img src="/img/point6.png" alt><br><img src="/img/point7.png" alt><br>SGD的变种主要围绕3个方向展开</p>
<ol>
<li><p>优化learn_rate → 自适应学习率</p>
<ul>
<li>Annealing 全局共享learn_rate 所有的参数以相同的幅度进行更新<ul>
<li>随步数衰减</li>
<li>指数衰减</li>
<li>1/t衰减</li>
</ul>
</li>
<li>Adagrad 参数独立learn_rate 更新幅度取决于参数本身</li>
</ul>
</li>
<li><p>优化梯度方向 → 方向感，减小震荡</p>
<ul>
<li>动量Momentum，Nesterov</li>
<li>SAG/SVRG/…</li>
</ul>
</li>
<li><p>并行化 → Scalable<br><img src="/img/point8.png" alt></p>
</li>
</ol>
<p><img src="/img/point_op0.png" alt><br><img src="/img/point_op1.gif" alt><br><img src="/img/point_op2.gif" alt><br><img src="/img/point_op3.gif" alt></p>
<h2 id="9-VC-Dimension"><a href="#9-VC-Dimension" class="headerlink" title="9. VC Dimension"></a>9. VC Dimension</h2><p>1943年，模拟神经网络由麦卡洛可（McCulloch）和皮茨（Pitts)提出，他们分析了理想化的人工神经元网络，并且指出了它们进行简单逻辑运算的机制。</p>
<p>1957年，康奈尔大学的实验心理学家弗兰克·罗森布拉特(Rosenblatt)在一台IBM–704计算机上模拟实现了一种他发明的叫作“感知机”（Perceptron）的神经网络模型。神经网络与支持向量机都源自于感知机（Perceptron）。</p>
<p>1962年，罗森布拉特著作：《神经动力学原理：感知机和大脑机制的理论》（Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms）。</p>
<p>1969年，明斯基和麻省理工学院的另一位教授佩普特合作著作：《感知机：计算几何学》（Perceptrons: An Introduction to Computational Geometry)。在书中，明斯基和佩普特证明单层神经网络不能解决XOR（异或）问题。</p>
<p>1971年，V. Vapnik and A. Chervonenkis在论文“On the uniform convergence of relative frequencies of events to their probabilities”中提出VC维的概念。</p>
<p>1974年，V. Vapnik提出了结构风险最小化原则。</p>
<p>1974年，沃波斯（Werbos）的博士论文证明了在神经网络多加一层，并且利用“后向传播”（Back-propagation）学习方法，可以解决XOR问题。那时正是神经网络研究的低谷，文章不合时宜。</p>
<p>1982年，在加州理工担任生物物理教授的霍普菲尔德，提出了一种新的神经网络，可以解决一大类模式识别问题，还可以给出一类组合优化问题的近似解。这种神经网络模型后被称为霍普菲尔德网络。</p>
<p>1986年，Rummelhart与McClelland发明了神经网络的学习算法Back Propagation。</p>
<p>1993年，Corinna Cortes和Vapnik等人提出了支持向量机(support vector machine)。神经网络是多层的非线性模型，支持向量机利用核技巧把非线性问题转换成线性问题。</p>
<p>1992~2005年，SVM与Neural network之争，但被互联网风潮掩盖住了。</p>
<p>2006年，Hinton提出神经网络的Deep Learning算法。Deep Learning假设神经网络是多层的，首先用Restricted Boltzmann Machine（非监督学习）学习网络的结构，然后再通过Back Propagation（监督学习）学习网络的权值。</p>
<p>现在，deep learning的应用越来越广泛，甚至已经有超越SVM的趋势。一方面以Hinton，Lecun为首的深度学习派坚信其有效实用性，另一方面Vapnik等统计机器学习理论专家又坚持着理论阵地，怀疑deep learning的泛化界。</p>
<h3 id="9-1-补充：解决过拟合"><a href="#9-1-补充：解决过拟合" class="headerlink" title="9.1 补充：解决过拟合"></a>9.1 补充：解决过拟合</h3><ol>
<li>特征选择或者降维</li>
<li>正则化</li>
<li>树：剪枝</li>
<li>深度学习：Dropout、BN</li>
<li>更多的数据，可以采集新数据，也可以数据增强与合成（可能改变数据分布）。</li>
<li>降低模型复杂度。</li>
<li>集成方法。</li>
</ol>
<h2 id="10-不平衡问题与AUC"><a href="#10-不平衡问题与AUC" class="headerlink" title="10. 不平衡问题与AUC"></a>10. 不平衡问题与AUC</h2><h3 id="10-1-AUC"><a href="#10-1-AUC" class="headerlink" title="10.1 AUC"></a>10.1 AUC</h3><table>
<thead>
<tr>
<th>$$$\hat{y}$$$ \ $$$y$$$</th>
<th>+</th>
<th>-</th>
</tr>
</thead>
<tbody>
<tr>
<td>+</td>
<td>TP</td>
<td>FP</td>
</tr>
<tr>
<td>-</td>
<td>FN</td>
<td>TN</td>
</tr>
</tbody>
</table>
<p>ROC曲线的横坐标是FPR（假阳性率），纵坐标是TPR（真阳性率）。其中，<br>$$FPR=\frac{FP}{TP+FN}$$<br>$$TPR=\frac{FP}{FP+TN}$$<br>以下解释绘制ROC曲线的过程。首先按照模型预测的样本为正例的概率排序，然后选一个截断点，该点之前的都是正例，后面的都是负例。理论上最优情况所有的整理应该排在负例的前面，此时的曲线就是紧贴左侧和上侧的一根曲线。绘制的过程中这么考虑，在最开始的时候，认为阈值在第一个样本之前，即所有的样本点都认为是负例，此时的TPR和FPE都是0，然后每次向后移动一个位置的阈值，如果预测正确了，就会有一个点从FN变到TP，此时TPR会增加一个单位，曲线向y轴正方向移动，如果预测错误，就会导致一个点从TN变到FP，此时曲线向x轴正方向移动。<br>相比于PR曲线，ROC对于不平衡样本不敏感，这一点对于PR曲线来说是很大的一个优势。另一个角度来说，ROC关注的是排序或者比较性能，比如说模型认为每一个样本是正类的概率都小于0.1，但是可以对正类预测的概率都大于负类，此时AUC是1，但是似然很低。<br>对于这种序关系的建模，可以使用RankSVM来建模。</p>
<p>参考：<a href="https://tracholar.github.io/machine-learning/2018/01/26/auc.html" target="_blank" rel="noopener">深入理解AUC</a></p>
<h3 id="10-2-不平衡问题"><a href="#10-2-不平衡问题" class="headerlink" title="10.2 不平衡问题"></a>10.2 不平衡问题</h3><p>说到ROC就要说到样本不平衡问题了，因为ROC是应对不平衡问题的一个重要的是指标。那么除此之外，还有哪些解决不平衡样本的方法？<br>首先先要明确不平衡问题的重要性：</p>
<ol>
<li>每年，约 2% 的信用卡账户是伪造的。（多数的欺诈检测领域是极其不平衡的）</li>
<li>针对某一病征的医学筛查通常涵盖了许多没有此病征的人，以检查出少数患者（例：美国的 HIV 感染率约为 0.4%）</li>
<li>每年，硬盘驱动器故障的发生率约为 1%</li>
<li>在线广告的转化率在 10^-3 到 10^-6 的范围区间内</li>
<li>工厂的产品缺陷率一般在 0.1% 左右</li>
</ol>
<p>这里不明确的给出方法，因为网上有很多总结的很好的文章。主要方法用过采样以及样本合成技术，欠采样以及集成技术，更合适的评价指标与损失函数。</p>
<h2 id="11-Matrix-Factorization"><a href="#11-Matrix-Factorization" class="headerlink" title="11. Matrix Factorization"></a>11. Matrix Factorization</h2><p>这一块谈谈自己的理解，矩阵分解技术是一种降维的强有力手段。包括常见降维中的PCA，NMF；主题模型中的LSA、pLSA；推荐系统中使用的LFM（Latent Factor Model）模型；再到自然语言处理中的word embedding技术，包括word2vec、GloVe等等，其中都有矩阵分解的影子。</p>
<h3 id="11-1-SVD"><a href="#11-1-SVD" class="headerlink" title="11.1 SVD"></a>11.1 SVD</h3><p>基于SVD的矩阵分解技术是一大票技术，包括PCA、autoencoder、LSA、LFM中使用的Funk-SVD。其中原理不详细解释，可以看PCA的那篇文章。LFM稍微不太一样，因为原本的效用矩阵中存在缺失，所以整体需要基于已知数据使用ALS进行损失函数的优化。</p>
<h3 id="11-2-主题模型"><a href="#11-2-主题模型" class="headerlink" title="11.2 主题模型"></a>11.2 主题模型</h3><p>包括最简单的LSA（1990），就是一个SVD换了一个马甲、然后是概率化版本的pLSA（1999）。然后是贝叶斯化的LDA，再有就是2005年提出的HDP（Hierarchical Dirichlet Process）。</p>
<p>简单写一下pLSA。原理也是认为有两个矩阵，一个是文档-主题矩阵，一个是主题-词矩阵，相比于LSA的优势在于为矩阵赋予了多项式分布的先验知识，更好的刻画一词多义。其中文档是一个隐变量，使用EM算法进行优化、E步是求解给定词和文档是，主题的后验概率，M步固定隐变量的后验概率，优化似然函数的下界。</p>
<p>LDA是认为多项式分布的参数不是固定的，是从狄利克雷分布中抽样得到的。没有看的太明白，回头再总结。</p>
<p>参考：<br><a href="https://blog.csdn.net/myproudcodelife/article/details/52094073" target="_blank" rel="noopener">主题模型TopicModel：Unigram、LSA、PLSA主题模型详解</a></p>
<h3 id="11-3-词嵌入"><a href="#11-3-词嵌入" class="headerlink" title="11.3 词嵌入"></a>11.3 词嵌入</h3><p>其实任何对词的表示都是一种嵌入，比如LSTM前面的自己训练的embedding层也是一种层面上的自己的词嵌入。不过如果有更好的词嵌入作为初始化或者直接用都是。举例来说，one-hot是词嵌入，转为主题向量也是词嵌入、而word2vec就是一种比较好的基于context prediction的预训练的嵌入模型、这里不做太多的解释，因为目前来说掌握的还算可以。但是自己一琢磨，word2vec可以从矩阵分解的角度来理解吗？这个问题我没有严格的推导，但是毫无疑问是可以的。</p>
<h3 id="11-4-非线性降维"><a href="#11-4-非线性降维" class="headerlink" title="11.4 非线性降维"></a>11.4 非线性降维</h3><h2 id="12-FM"><a href="#12-FM" class="headerlink" title="12. FM"></a>12. FM</h2><p>TODO<br>参考：<a href="https://blog.csdn.net/itplus/article/details/40534923" target="_blank" rel="noopener">FM</a></p>
<p>基于共线矩阵的词向量<br>参考：<br><a href="https://blog.csdn.net/u014665013/article/details/79642083" target="_blank" rel="noopener">链接1</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="https://www.cwzj.com/archives/2602" target="_blank" rel="noopener">机器学习五要素 optimization</a></li>
<li><a href="http://www.cnblogs.com/neopenx/p/4768388.html" target="_blank" rel="noopener">优化方法——大魔导书</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21798784" target="_blank" rel="noopener">CS231n课程笔记翻译：神经网络笔记3（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21539419" target="_blank" rel="noopener">《An overview of gradient descent optimization algorithms》</a></li>
<li><a href="http://www.datakit.cn/blog/2016/07/04/sgd_01.html" target="_blank" rel="noopener">随机梯度下降综述 </a></li>
<li><a href="http://blog.sina.com.cn/s/blog_5eef0840010148l7.html" target="_blank" rel="noopener">SVM训练算法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32784067" target="_blank" rel="noopener">OCSVM</a></li>
<li><a href="https://www.shintaku.cc/posts/deepwalk/" target="_blank" rel="noopener">Deepwalk论文详解</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://protao.github.io/2018/08/17/机器学习-2018-08-17-point/" data-id="cjxo5e6sw004dz16dsvbgmlxl" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/algorithm/">algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/machinelearning/">machinelearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="../../../../tags/model/">model</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../../22/机器学习-2018-08-22-RecommenderSystemsInAction/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          《推荐系统实战》读书笔记
        
      </div>
    </a>
  
  
    <a href="../../16/程序员的玩具-2018-08-16-PriorityQueue/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">从优先队列到贪心算法</div>
    </a>
  
</nav>

  
</article>




</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="../../../../categories/C/">C++</a><span class="category-list-count">25</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/python/">python</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/信息安全/">信息安全</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/大数据/">大数据</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/数学/">数学</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/机器学习/">机器学习</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/生活/">生活</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/程序员的玩具/">程序员的玩具</a><span class="category-list-count">38</span></li><li class="category-list-item"><a class="category-list-link" href="../../../../categories/读书笔记/">读书笔记</a><span class="category-list-count">7</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/C/">C++</a><span class="tag-list-count">23</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/algorithm/">algorithm</a><span class="tag-list-count">34</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/bigdata/">bigdata</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/database/">database</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/dataprocessing/">dataprocessing</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/deeplearning/">deeplearning</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/financing/">financing</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/generative/">generative</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/hadoop/">hadoop</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/hash/">hash</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/hbase/">hbase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/linux/">linux</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/machinelearning/">machinelearning</a><span class="tag-list-count">22</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/maths/">maths</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/model/">model</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/mysql/">mysql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/nlp/">nlp</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/numpy/">numpy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/python/">python</a><span class="tag-list-count">26</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/reading/">reading</a><span class="tag-list-count">38</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/scala/">scala</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/security/">security</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/shell/">shell</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/spark/">spark</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/spider/">spider</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/tools/">tools</a><span class="tag-list-count">23</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/translation/">translation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/trick/">trick</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="../../../../tags/web/">web</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="../../../../tags/C/" style="font-size: 17.69px;">C++</a> <a href="../../../../tags/algorithm/" style="font-size: 19.23px;">algorithm</a> <a href="../../../../tags/bigdata/" style="font-size: 15.38px;">bigdata</a> <a href="../../../../tags/database/" style="font-size: 10px;">database</a> <a href="../../../../tags/dataprocessing/" style="font-size: 12.31px;">dataprocessing</a> <a href="../../../../tags/deeplearning/" style="font-size: 13.85px;">deeplearning</a> <a href="../../../../tags/financing/" style="font-size: 11.54px;">financing</a> <a href="../../../../tags/generative/" style="font-size: 12.31px;">generative</a> <a href="../../../../tags/hadoop/" style="font-size: 12.31px;">hadoop</a> <a href="../../../../tags/hash/" style="font-size: 12.31px;">hash</a> <a href="../../../../tags/hbase/" style="font-size: 10px;">hbase</a> <a href="../../../../tags/linux/" style="font-size: 13.85px;">linux</a> <a href="../../../../tags/machinelearning/" style="font-size: 16.92px;">machinelearning</a> <a href="../../../../tags/maths/" style="font-size: 16.15px;">maths</a> <a href="../../../../tags/model/" style="font-size: 11.54px;">model</a> <a href="../../../../tags/mysql/" style="font-size: 10px;">mysql</a> <a href="../../../../tags/nlp/" style="font-size: 14.62px;">nlp</a> <a href="../../../../tags/numpy/" style="font-size: 10px;">numpy</a> <a href="../../../../tags/python/" style="font-size: 18.46px;">python</a> <a href="../../../../tags/reading/" style="font-size: 20px;">reading</a> <a href="../../../../tags/scala/" style="font-size: 10px;">scala</a> <a href="../../../../tags/security/" style="font-size: 13.85px;">security</a> <a href="../../../../tags/shell/" style="font-size: 13.08px;">shell</a> <a href="../../../../tags/spark/" style="font-size: 10.77px;">spark</a> <a href="../../../../tags/spider/" style="font-size: 10px;">spider</a> <a href="../../../../tags/tools/" style="font-size: 17.69px;">tools</a> <a href="../../../../tags/translation/" style="font-size: 10.77px;">translation</a> <a href="../../../../tags/trick/" style="font-size: 12.31px;">trick</a> <a href="../../../../tags/web/" style="font-size: 11.54px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2019/06/">六月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2019/05/">五月 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2019/04/">四月 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2019/03/">三月 2019</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2019/02/">二月 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2019/01/">一月 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/12/">十二月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/11/">十一月 2018</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/10/">十月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/09/">九月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/08/">八月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/07/">七月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/06/">六月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/05/">五月 2018</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/04/">四月 2018</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/03/">三月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/02/">二月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2018/01/">一月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/11/">十一月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/10/">十月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/09/">九月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/08/">八月 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/07/">七月 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/06/">六月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/05/">五月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/04/">四月 2017</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="../../../../archives/2017/03/">三月 2017</a><span class="archive-list-count">5</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="../../../../2019/06/17/生活-2019-06-17-GTD/">《搞定I——无压工作的艺术》</a>
          </li>
        
          <li>
            <a href="../../../../2019/05/26/C-2019-05-26-Effective-CPP-IV/">《Effective C++》第四部分：设计和声明</a>
          </li>
        
          <li>
            <a href="../../../../2019/05/17/C-2019-05-17-Effective-CPP-III/">《Effective C++》第三部分：资源管理</a>
          </li>
        
          <li>
            <a href="../../../../2019/05/10/机器学习-2019-05-10-alchemy-trick/">仓鼠一般搜集到的炼丹技巧</a>
          </li>
        
          <li>
            <a href="../../../../2019/05/08/Python-2019-05-08-SICP2/">Python中使用函数构建对象</a>
          </li>
        
      </ul>
    </div>
  </div>

  


  </span>
</aside>

        
      </div>
      <footer id="footer">

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>

  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Yongtao Zhang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="../../../../index.html" class="mobile-nav-link">home</a>
  
    <a href="../../../../archives" class="mobile-nav-link">archives</a>
  
    <a href="../../../../about" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="../../../../fancybox/jquery.fancybox.css">
  <script src="../../../../fancybox/jquery.fancybox.pack.js"></script>


<script src="../../../../js/script.js"></script>



  </div>
</body>
</html>